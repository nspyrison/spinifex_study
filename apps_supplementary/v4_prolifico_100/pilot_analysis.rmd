---
title: "prolifico 100"
author: "Nick Spyrison"
date: "18/02/2021"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
rel_path <- function(rel_path = "."){
  rel_dir  <- dirname(rstudioapi::getSourceEditorContext()$path)
  rel_path <- paste0(rel_dir, "/", rel_path)
  normalizePath(rel_path, winslash = "/")
}
rel_path()

## Read from gsheets API4 and save local
if(F){
  ss_id <- "1K9qkMVRkrNO0vufofQJKWIJUyTys_8uVtEBdJBL_DzU" ## Hash id of the google sheet
  raw <- googlesheets4::read_sheet(ss_id, sheet = 1L)
  raw <- raw %>% filter(!is.na(plot_active)) # Remove dummy rows with application notes
  ## Doesn't work in mutate:
  raw$full_perm_num = unlist(as.integer(raw$full_perm_num))
  raw$prolific_id = unlist(as.character(raw$prolific_id))
  str(raw)
  saveRDS(raw, "./apps_supplementary/v4_prolifico_100/data/raw_prolific_100.rds")
}
## Load load and clean, save cleaned
if(F){
  raw <- readRDS(rel_path("data/raw_prolific_100.rds"))
  ## Filter to only task data including training
  dat_active <- raw %>%  filter(plot_active == TRUE)
    
  ## Pivot_longer, many variable columns 
  #source("./paper/R/pivot_longer_resp_ans_tbl.r")
  source(file = rel_path("../../paper/R/pivot_longer_resp_ans_tbl.r"), local = T, echo = T)
  dat_longer <- dat_active %>% pivot_longer_resp_ans_tbl()
  .mean_diff <- mean(dat_longer$sec_on_pg, na.rm = TRUE) - mean(dat_longer$sec_to_resp, na.rm = TRUE)
  dat_longer <- dat_longer %>% ## Impute missing sec_to_resp
    mutate(sec_to_resp = dplyr::if_else(is.na(sec_to_resp), sec_on_pg - .mean_diff, sec_to_resp))
  ## Aggregate to task grain.
  dat_task_agg <- aggregate_task_vars(dat_longer)
  dat_task_agg <- dat_task_agg %>%
    dplyr::mutate(task_input_inter = dplyr::if_else(
      factor == "radial", task_input_inter -1L, task_input_inter))
  saveRDS(dat_task_agg, rel_path("data/dat_task_agg_prolific_100.rds"))
}
## load aggregated data.
dat_task_agg <- readRDS(rel_path("data/dat_task_agg_prolific_100.rds"))
my_theme <- list(
  theme_minimal(),
  scale_color_brewer(palette = "Dark2"),
  scale_fill_brewer(palette = "Dark2"),
  geom_hline(yintercept = 0L)
)
my_ggplot <- function(.aes = aes(x = factor, y = task_marks,
                                 color = is_training, fill = is_training),
                      .title = "Default title",
                      .data = dat_qual){
  ggplot(.data, .aes) +
    labs(title = .title) +
    my_theme +
    geom_point(
      position = position_jitterdodge(jitter.width = .3, jitter.height = .05), alpha = .2) +
    geom_boxplot( position = "dodge", alpha = .4)
}
```

______

## Quality of data

I am concerned about the quality of the data, given that so many people experienced network issues. Let's explore.

```{r}
lb <- quantile(dat_task_agg$max_sec_to_resp, probs = .25) ## bottom 25 %
ub <- quantile(dat_task_agg$max_sec_to_resp, probs = .98) ## top 2 %
ggplot() + 
  geom_density(aes(max_sec_to_resp, fill = ""), dat_task_agg, alpha = .5) +
  ggtitle("Seconds to respond density") +
  my_theme +
  lims(x = c(0, 100)) +
  geom_vline(xintercept = c(lb, ub), linetype = 2L) +
  geom_label(aes(x = c(lb + 12, ub - 12), y = c(.05, .05), 
                 label = c(".25 percentile", ".98 percentile"))) +
  annotate("rect", xmin = -Inf, xmax = lb, ymin = -Inf, ymax = Inf,
           alpha = 0.3, fill = "firebrick1") +
  annotate("rect", xmin = lb, xmax = ub, ymin = -Inf, ymax = Inf,
           alpha = 0.3, fill = "aquamarine") +
  annotate("rect", xmin = ub, xmax = Inf, ymin = -Inf, ymax = Inf,
           alpha = 0.3, fill = "firebrick1")

print("!! For the rest of the analysis we will be looking at this middle chunk of data!!")
dat_qual <- dat_task_agg %>% filter(max_sec_to_resp > lb, max_sec_to_resp < ub)


obs_frac <- round(100L * nrow(dat_qual) / nrow(dat_task_agg), 2)
paste0("Percent of original data in subset: ", obs_frac, "%")

#sub_lb <- dat_task_agg %>% filter(max_sec_to_resp > lb)
#nrow(sub_lb) / nrow(dat_task_agg) #technically bottom 28.35% atm

### only look at eval data for correlation.
# cor_all  <- round(cor(dat_task_agg$max_sec_to_resp, dat_task_agg$task_marks), 3L)
# cor_qual <- round(cor(dat_qual$max_sec_to_resp, dat_qual$task_marks), 3L)
# paste0("cor(max_sec ~ task_marks) (all): ", cor_all)
# paste0("cor(max_sec ~ task_marks) (remove bottom 25%, top .5%): ", cor_qual)

```

```{r}
ggplot(dat_qual) + my_theme +
  geom_point(aes(max_sec_to_resp, task_marks,
                 fill = is_training, color = is_training), alpha = .2) +
  geom_smooth(aes(max_sec_to_resp, task_marks,
                  fill = is_training, color = is_training)) +
  ggtitle("Performance by last response time",
          subtitle = "after about 25 seconds marks decrease (difficulty? low attention?)")
```


## Even-ness of evaluation

Let's look at the distribution of quality evaluations by full_perm_num.

```{r}
l_u_ids <- dat_qual$prolific_id %>% unique() %>% length() ## ~54
id_evals <- round(nrow(dat_qual) / 9, 1) ## 67.2

u_prolific_ids <- dat_qual$prolific_id %>% unique()
u_prolific_ids <- 
  u_prolific_ids[!u_prolific_ids %in% c(NULL, "NULL" ,"", "DVIA or numbat")] 
l_u_prolific_ids <- u_prolific_ids  %>% length() ## ~ 52
prolific_evals <- 
  round(nrow(dat_qual[dat_qual$prolific_id %in% u_prolific_ids, ]) / 9, 1)
paste0(l_u_prolific_ids, " unique prolific ids, with about ", prolific_evals, " evaluations within bounds.")

## perm_num_agg
perm_num_agg <- dat_qual %>% group_by(full_perm_num, is_training) %>%
  summarise(n = n(),
            even_studies = max(sum(is_training) / 3, sum(!is_training) / 6),
            cnt_prolific_id = length(unique(prolific_id))
  ) %>% ungroup() %>% 
  mutate(is_past_tgt = if_else(even_studies > 3, TRUE, FALSE))

nums_past_tgt <- perm_num_agg$full_perm_num[perm_num_agg$is_past_tgt] %>% 
  as.integer() %>% unique() # c(6, 16, 18, 24, 25, 26, 28, 29) #atm

.mn_val <- mean(perm_num_agg$even_studies)
ggplot(perm_num_agg, aes(x = full_perm_num, y = even_studies, 
                         color = is_training, fill = is_training)) +
  labs(title = "Evaluations by permutaion", 
       subtitle = "Though not from so many participants") +
  my_theme + 
  geom_bar(stat = "identity", position = "dodge") + 
  geom_hline(yintercept = .mn_val, linetype = 3) +
  geom_hline(yintercept = 3, linetype = 2) + 
  geom_text(aes(x = 30, y = 3.2, label = "target"), color = "black") +
  geom_text(aes(x = 28, y = .mn_val + .2, label = "current mean"), color = "black")
```

```{r}
## prolific_id_agg
prolific_id_agg <- dat_qual %>% group_by(prolific_id , is_training) %>%
  summarise(n = n(),
            even_studies = max(sum(is_training) / 3, sum(!is_training) / 6),
            cnt_participant_num = length(unique(participant_num))
  ) %>% ungroup() 
prolific_id_agg <- prolific_id_agg[order(prolific_id_agg$even_studies, decreasing = T),]

ggplot(prolific_id_agg, aes(x = prolific_id, y = even_studies, 
                         color = is_training, fill = is_training)) +
  labs(title = "Evaluations by prolific_id") +
  my_theme + 
  geom_bar(stat = "identity", position = "dodge") + geom_hline(yintercept = 1) +
  geom_text(aes(x = .95 * max(length(unique(prolific_id_agg$prolific_id))), y = 1.2, 
                label = "ideal"), color = "black")

print("highest number of evals per person:")
head(prolific_id_agg)
```



## Marks by everything

```{r}
my_ggplot(aes(x = factor, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by factor", 
          .data = dat_qual)

my_ggplot(aes(x = eval, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by order of evaluation",
          .data = dat_qual)

my_ggplot(aes(x = vc, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by var-covariance",
          .data = dat_qual)

my_ggplot(aes(x = p_dim, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by order of dimension",
          .data = dat_qual)

my_ggplot(aes(x = location, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by order of location",
          .data = dat_qual)
```

______

## Speed by factor, order

```{r}
my_ggplot(aes(x = factor, y = max_sec_to_resp,
              color = is_training, fill = is_training),
          "Seconds till last response by factor",
          .data = dat_qual) +
  coord_cartesian(ylim = c(0L, 70L))

my_ggplot(aes(x = eval, y = max_sec_to_resp,
              color = is_training, fill = is_training),
          "Seconds till last response by evaluation order",
          .data = dat_qual) +
  coord_cartesian(ylim = c(0L, 70L))
```

______

## Input and response interactions

```{r}
my_ggplot(aes(x = factor, y = task_input_inter,
              color = is_training, fill = is_training),
          "Number of input interaction (throughness) by factor",
          .data = dat_qual) +
  coord_cartesian(ylim = c(0L, 30L))

my_ggplot(aes(x = factor, y = task_resp_inter,
              color = is_training, fill = is_training),
          "Number of response interaction (inverse confidence) by factor",
          .data = dat_qual)
```



