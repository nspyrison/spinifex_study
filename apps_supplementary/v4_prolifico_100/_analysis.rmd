---
title: "prolifico 100"
author: "Nick Spyrison"
date: "11/03/2021"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
header-includes:
   - \usepackage{amsmath}
   - \usepackage{showframe}
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
library(tidyverse)
library("lme4")
library("merTools")

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
rel_path <- function(rel_path = "."){
  rel_dir  <- dirname(rstudioapi::getSourceEditorContext()$path)
  rel_path <- paste0(rel_dir, "/", rel_path)
  normalizePath(rel_path, winslash = "/")
}

## Read from gsheets API4 and save local
if(F){
  ## Hash id of the google sheet
  ss_id <- "1K9qkMVRkrNO0vufofQJKWIJUyTys_8uVtEBdJBL_DzU" 
  raw <- googlesheets4::read_sheet(ss_id, sheet = 1L)
  ## Remove dummy rows
  raw <- raw %>% filter(!is.na(plot_active), ## dummy rows
                        !is.na(participant_num)) ## 2 missing participant_nums?
  ## Format 
  raw$full_perm_num = unlist(as.integer(raw$full_perm_num))
  raw$prolific_id = unlist(as.character(raw$prolific_id))
  dim(raw)
  saveRDS(raw, "./apps_supplementary/v4_prolifico_100/data/raw_prolific_100.rds")
}
## Load load and clean, save cleaned
if(F){
  raw <- readRDS(rel_path("data/raw_prolific_100.rds"))
  ## Only plot_active rows 
  ## AND Only prolific_ids (rows where nchar(prolific_id) == 24 charcters long)
  dat_active <- raw %>% filter(plot_active == TRUE,
                        nchar(prolific_id) == 24L)
  source(file = rel_path("../../paper/R/clean_participant_data.r"),
         local = T, echo = F)
  ## pivot variables columns longer for task-grained aggregation.
  dat_longer <- dat_active %>% pivot_longer_resp_ans_tbl()
  ## Mean impute missing sec_to_resp
  .mean_diff <- mean(dat_longer$sec_on_pg, na.rm = TRUE) -
    mean(dat_longer$sec_to_resp, na.rm = TRUE)
  dat_longer <- dat_longer %>%
    mutate(sec_to_resp = dplyr::if_else(is.na(sec_to_resp), 
                                        sec_on_pg - .mean_diff, sec_to_resp))
  ## Aggregate to task grain.
  dat_task_agg <- aggregate_task_vars(dat_longer)
  ## Fix observeEvent() over count of radial input_inter. 
  dat_task_agg <- dat_task_agg %>% 
    dplyr::mutate(task_input_inter = dplyr::if_else(
      factor == "radial", task_input_inter -1L, task_input_inter))
  ## Save task aggregated data.
  saveRDS(dat_task_agg, rel_path("data/dat_task_agg_prolific_100.rds"))
}
## load aggregated data.
dat_task_agg <- readRDS(rel_path("data/dat_task_agg_prolific_100.rds"))

## Local functions -----
## For labeling n, mean on boxplots, following:
#### https://medium.com/@gscheithauer/how-to-add-number-of-observations-to-a-ggplot2-boxplot-b22710f7ef80
## TODO: Has issue with non offseting the dodged colors.
stat_box_data <- function(y, lower_bound = min(-1, .data$.x_col) * 1.15) {
  data.frame(
    y = 0.95 * lower_bound,
    label = paste('n =', length(y), '\n',
                  'mean =', round(mean(y), 1), '\n')
  )
}
# ggplot(iris, aes(Species, Sepal.Length)) + 
#   geom_boxplot() +
#   stat_summary(
#     fun.data = stat_box_data, 
#     geom = "text", 
#     hjust = 0.5,
#     vjust = 0.9
#   )
my_theme <- list(
  theme_minimal(),
  scale_color_brewer(palette = "Dark2"),
  scale_fill_brewer(palette = "Dark2"),
  geom_hline(yintercept = 0L)
)
my_ggplot <- function(.aes = aes(x = eval, y = task_marks,
                                 color = is_training, fill = is_training),
                      .title = "Default title",
                      .data = dat_qual)
{
  ggplot(.data, .aes) +
    labs(title = .title) +
    my_theme +
    geom_point(
      position = position_jitterdodge(jitter.width = .3, jitter.height = .05), alpha = .2) +
    geom_boxplot(position = "dodge", alpha = .4)
}
my_ggplot2 <- function(.x_col,
                       .y_col,
                       .data = dat_qual,
                       .title = "Default title",
                       .aes = aes(x = {{.x_col}}, y = {{.y_col}},
                                  color = is_training, fill = is_training))
{
  ggplot(.data, .aes) +
    labs(title = .title) +
    my_theme +
    geom_point(
      position = position_jitterdodge(jitter.width = .3, jitter.height = .05), alpha = .2) +
    geom_boxplot(position = "dodge", alpha = .4) #+
  # stat_summary(
  #     fun.data = stat_box_data, 
  #     geom = "text", 
  #     hjust = 0.5,
  #     vjust = 0.9
  #   )
}
```

______

## Quality of data

I am concerned about the quality of the data, given that so many people experienced network issues. Let's explore.

```{r}
print(paste0("N_raw = ", length(unique(dat_task_agg$prolific_id)), " unique prolific ids"))
lb <- quantile(dat_task_agg$max_sec_to_resp, probs = .25) ## bottom 25 %
ub <- quantile(dat_task_agg$max_sec_to_resp, probs = .95) ## top 5 %
ggplot() + 
  geom_density(aes(max_sec_to_resp, fill = ""), dat_task_agg, alpha = .5) +
  ggtitle("Seconds to respond density") +
  my_theme +
  lims(x = c(0, 100)) +
  geom_vline(xintercept = c(lb, ub), linetype = 2L) +
  geom_vline(xintercept = 60, linetype = 3L) +
  geom_label(aes(x = c(lb + 6.5, 60 - 6.5, ub - 6.5), y = c(.05, .045, .05), 
                 label = c("25th percentile", "plot turns off" ,"95th percentile"))) +
  annotate("rect", xmin = -Inf, xmax = lb, ymin = -Inf, ymax = Inf,
           alpha = 0.3, fill = "firebrick1") +
  annotate("rect", xmin = lb, xmax = ub, ymin = -Inf, ymax = Inf,
           alpha = 0.3, fill = "aquamarine") +
  annotate("rect", xmin = ub, xmax = Inf, ymin = -Inf, ymax = Inf,
           alpha = 0.3, fill = "firebrick1")


dat_qual <- dat_task_agg %>% filter(max_sec_to_resp > lb, max_sec_to_resp < ub)
print(paste0("N_windsorized = ", length(unique(dat_qual$prolific_id)), " unique prolific ids"))
obs_percent <- round(100L * nrow(dat_qual) / nrow(dat_task_agg), 2)
paste0("Percent of original task evaluations in subset: ", obs_percent, "%")
print("NOTE: The rest of the analysis we will be looking at this middle ~70% of the evaluations.")
```

```{r}
ggplot(dat_qual) + my_theme +
  geom_point(aes(max_sec_to_resp, task_marks,
                 fill = is_training, color = is_training), alpha = .2) +
  geom_smooth(aes(max_sec_to_resp, task_marks,
                  fill = is_training, color = is_training)) +
  ggtitle("Performance by last response time",
          subtitle = "after about 25 seconds marks decrease (difficulty? low attention?)")
```


## Even-ness of evaluation

Let's look at the distribution of quality evaluations by full_perm_num.

```{r}
l_u_ids <- dat_qual$prolific_id %>% unique() %>% length() ## ~54
id_evals <- round(nrow(dat_qual) / 9, 1) ## 67.2

u_prolific_ids <- dat_qual$prolific_id %>% unique()
l_u_prolific_ids <- u_prolific_ids  %>% length() ## ~ 52
prolific_evals <- 
  round(nrow(dat_qual[dat_qual$prolific_id %in% u_prolific_ids, ]) / 9, 1)
paste0(l_u_prolific_ids, " unique prolific ids, with about ", prolific_evals, " evaluations after Windsorizing(.25, .95).")

## perm_num_agg
perm_num_agg <- dat_qual %>% group_by(full_perm_num, is_training) %>%
  summarise(n = n(),
            even_studies = max(sum(is_training) / 3, sum(!is_training) / 6),
            cnt_prolific_id = length(unique(prolific_id))
  ) %>% ungroup() %>% 
  mutate(is_past_tgt = if_else(even_studies > 3, TRUE, FALSE))

nums_past_tgt <- perm_num_agg$full_perm_num[perm_num_agg$is_past_tgt] %>%
  as.integer() %>% unique() # c(6, 16, 18, 24, 25, 26, 28, 29) #atm

.mn_val <- mean(perm_num_agg$even_studies)
ggplot(perm_num_agg, aes(x = full_perm_num, y = even_studies, 
                         color = is_training, fill = is_training)) +
  labs(title = "Evaluations by permutaion", 
       subtitle = "Though not from so many participants") +
  my_theme + 
  geom_bar(stat = "identity", position = "dodge") + 
  geom_hline(yintercept = .mn_val, linetype = 3) +
  geom_hline(yintercept = 3, linetype = 2) + 
  geom_text(aes(x = 30, y = 3.2, label = "target"), color = "black") +
  geom_text(aes(x = 28, y = .mn_val + .2, label = "current mean"), color = "black")
```

```{r}
## prolific_id_agg
prolific_id_agg <- dat_qual %>% group_by(prolific_id , is_training) %>%
  summarise(n = n(),
            even_studies = max(sum(is_training) / 3, sum(!is_training) / 6),
            cnt_participant_num = length(unique(participant_num))
  ) %>% ungroup() 
prolific_id_agg <- prolific_id_agg[order(prolific_id_agg$even_studies, decreasing = T),]

ggplot(prolific_id_agg, aes(x = prolific_id, y = even_studies, 
                         color = is_training, fill = is_training)) +
  labs(title = "Evaluations by prolific_id") +
  my_theme + 
  geom_bar(stat = "identity", position = "dodge") + geom_hline(yintercept = 1) +
  geom_text(aes(x = .95 * max(length(unique(prolific_id_agg$prolific_id))), y = 1.2, 
                label = "ideal"), color = "black")

print("highest number of evals per person:")
head(prolific_id_agg)
```


## Within-participants -- Marks by Evaluation order

```{r}
## Create a global unique id (guid), doesn't fix the issue, because the double counted t3 was from repeat 1_1 runs, on mar 4, remove it.

## instead we'll apply a simple filter
dat_qual_wi_participant <- dat_qual %>%
  filter(task_marks > -1L, task_marks < 1L) %>% 
  mutate(frame_pro_id = as.integer(prolific_id))

  ggplot(dat_qual_wi_participant, 
            aes(x = eval, y = task_marks,
                frame = frame_pro_id,
                group = frame_pro_id,
                color = is_training, fill = is_training)
  ) +
  labs(title = "Within Participants, Marks by evaluation order") +
  scale_color_brewer(palette = "Dark2") + 
  scale_fill_brewer(palette = "Dark2") +
    geom_point(aes(size = cnt_resp), alpha = .2,
               position = position_jitterdodge(jitter.width = .8,
                                               jitter.height = .05)
  ) +
  geom_line(alpha = .2)


## using jitter, in geom_lines doesn't perform the same offset as geom_point.

cor_sec_to_resp <- cor(dat_qual_wi_participant$max_sec_to_resp,
                       dat_qual_wi_participant$task_marks) %>% round(3)
cor_eval <- cor(as.integer(dat_qual_wi_participant$eval),
                dat_qual_wi_participant$task_marks) %>% round(3)
cor_cnt_resp <- cor(dat_qual_wi_participant$cnt_resp, 
                    dat_qual_wi_participant$task_marks) %>% round(3)

print(paste0(
  "Quite busy, looking at the correlation, cor(eval number, marks) = ", cor_eval,
  ", a moderate negative value given that, cor(max time to respond, marks) = ", 
  cor_sec_to_resp
))

print("Tried animating over participants, but erroring (Error in -data$group : invalid argument to unary operator) on a valid ggplot. Would be across the board with such low correlation")


## Reviewing correlations, nothing too exciting, 
# corr_dat <- dat_qual %>%
#   mutate(eval = as.numeric(eval)) %>% 
#   dplyr::select(!z_weight_check)
# num_col_ind <- unlist(lapply(corr_dat, is.numeric))
# car_dat_mat <- corr_dat[, num_col_ind] %>% as.matrix() %>% cor()
# col3 <- colorRampPalette(c("blue", "white", "red"))
# corrplot::corrplot.mixed(car_dat_mat,
#                          lower.col = col3(100),
#                          upper.col = col3(100))
```


## Random effects regression model 

We'll first consider the baseline fixed effects considering only the continuous variables. We have:

$$
\begin{align*}
\widehat{marks} = ~
&\beta_{int} +
\beta_{SecResp} * SecResp +
\beta_{SecPg} * SecPg + 
\beta_{RespInter} * RespInter ~+ \\
&\beta_{CntResp} * CntResp +
\beta_{InputInter} * InputInter +
\epsilon
\end{align*}
$$
Where, 
$$\epsilon \sim \mathcal{N}(0,~\sigma)$$

Building on this, we add in vectors of all of our fixed factor terms: method factor, data dimension, variance-covariance shape, signal mixing location. Add a factor interaction to the number of input interactions as it is it intrinsically different based on the factor. Lastly we add the random effect of the participants. This random term, $effect_{participant}$, accounts for the difference between participant, that is the each individuals ability/disposition to effect their task marks. Then our mixed model is: 

$$
\begin{align*}
\widehat{marks} = ~
&\beta_{int} +
\beta_{SecResp} * SecResp +
\beta_{SecPg} * SecPg + 
\beta_{RespInter} * RespInter + \\
&\beta_{CntResp} * CntResp + 
\boldsymbol{\beta}_{InputInter*Factor} * \boldsymbol{InputInter} * I(\boldsymbol{Factor}) +
\boldsymbol{\beta}_{factor} * I(\boldsymbol{factor}) + \\
&\boldsymbol{\beta}_{dim} * I(\boldsymbol{dim}) + 
\boldsymbol{\beta}_{shape} * I(\boldsymbol{shape}) + 
\boldsymbol{\beta}_{location} * I(\boldsymbol{location}) + 
\boldsymbol{\beta}_{participant} * \boldsymbol{effect}_{participant} +
\epsilon
\end{align*}
$$
Where,  
$$
\begin{align*}
&\epsilon \sim \mathcal{N}(0,~\sigma) \\
&effect_{participant_i} \sim \mathcal{N}(0,~\sigma_{participant_i}) ~|~ i \in level(participant~ids)  \\
&factor \in (pca,~grand,~radial) \\
&dim \in (4,~6) \text{ variables, with 3 & 4 clusters  respectivly} \\
&shape \in (EEE,~EEV,~EVV~banana) \\
&location \in (0/100,~33/66,~50/50) \text{ % mixing of a noise and signal variable respectively} \\
&I() \text{ is the indicator function, a logical value for each level of the fixed factor}
&\end{align*}
$$



```{r}
## Mixed (fixed and random/variable) effects regression model,
### following along with:
if(F) 
  browseURL("https://m-clark.github.io/mixed-models-with-R/random_intercepts.html#running-a-mixed-model")
#install.packages("lme4")

dat_qual_onlyeval_renamed <- dat_qual %>% filter(is_training == FALSE) %>%
  rename(shape = vc, dim = p_dim, order = eval, participant = prolific_id)

## The Models:

### 1) Start with simple terms of all variables

marks_lmer1 <- lmer( ## 15 terms remain after dropping 3 due to rank deficiency
  task_marks ~ 
    max_sec_to_resp + max_sec_on_pg + task_resp_inter + cnt_resp + task_input_inter * factor + ## Numeric, input_inter*factor interaction
    factor + dim + shape + location + order + ## Fixed factor random terms
    (1 | participant), ## Random effect
  data = dat_qual_onlyeval_renamed
)
aic_1 <- AIC(marks_lmer1)
print(paste0("Model 1) simple terms of all variables. 16 terms remain after dropping 4 due to rank deficiency, AIC = ", 
             round(aic_1, 0), "."))

### 2) remove the least bang for buck, 3 variables that are highly correlated, lose .1% AIC
marks_lmer2 <- lmer( ## 13 terms remain after dropping 3 due to rank deficiency
  task_marks ~
    max_sec_to_resp + task_resp_inter + task_input_inter * factor + ## Numeric
    factor + dim + shape + location + order + ## Fixed factor random terms
    (1 | participant), ## Random effect
  data = dat_qual_onlyeval_renamed
)
aic_2 <- AIC(marks_lmer2)
print("Model 2) simplify by removing 2 of the most correlated (redundant explaintion) numeric variables. We gain ~.2% AIC to remove 2 terms relative to the first model.")

### 3) Try to mix all terms of the simple model with factor
print("Model 3) we try to make an over complex model by starting with model 2, leave independant terms and introduce factor interactions. This costs 22 terms to improve AIC by 6.5%")
if(F){
  marks_lmer3 <- lmer( ## 35 terms remain after dropping 10 due to rank deficiency, 
    task_marks ~
      ## Independent
      max_sec_to_resp + task_resp_inter + task_input_inter + ## Numeric
      factor + dim * factor + shape + location + order + ## Fixed factor random terms
      (1 | participant) +
      ## factor interactions
      factor * (max_sec_to_resp + max_sec_on_pg + task_resp_inter + task_input_inter) + ## Numeric
      factor * (dim  + shape  + location + order) + ## Fixed factor random terms
      factor * (1 | participant), ## Random effect
    data = dat_qual_onlyeval_renamed
  )
  aic_3 <- AIC(marks_lmer3)
  
  # print("Confidence intervals:")
  # confint(marks_lmer)
  # 
  # print("Head of participant effect:")
  # coef(marks_lmer)$participant %>% head(5)
  # 
  # predictInterval(marks_mixed_mod)   # for various model predictions, possibly with new data
  # REsim(marks_mixed_mod)             # mean, median and sd of the random effect estimates
}

print("Let's take a closer look at model 2: \n summary:")
## Model summary
print(summary(marks_lmer2), digits = 3)

## Residual plot
resid_2 <- data.frame(predicted  = predict(marks_lmer2), residual = residuals(marks_lmer2),
                      factor = dat_qual_onlyeval_renamed$factor, cnt_resp = dat_qual_onlyeval_renamed$cnt_resp)
p <- ggplot(resid_2, aes(x = predicted, y = residual, color = factor, size = cnt_resp)) + 
  geom_point(alpha = .2) + my_theme + ggtitle("Residuals by predcted values")
ggExtra::ggMarginal(p, type = "density", fill = "grey80")

## Random effects, as simulated from model posterior distributions
print("Random effect of participant, as simulated from the model posterior distributions")
plotREsim(REsim(marks_lmer2))  # plot the interval estimates

## Anova table
anova(marks_lmer2) ## do we need t interact task_input_inter with factor (task_input_inter doesn't make sense for grand)?
```


## Marks by ... 

### Factor

```{r}
my_ggplot2(.x_col = factor, 
          .y_col = task_marks,
          .title = "Marks by factor")
```

### Evaluation order

```{r}
my_ggplot(.aes = aes(x = eval, y = task_marks,
                     color = is_training, fill = is_training),
          .title = "Marks by evaluation order",
          .data = dat_qual)
```

### Covariance

```{r} 
my_ggplot(aes(x = vc, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by var-covariance",
          .data = dat_qual)
```

### Dimension

```{r}
my_ggplot(aes(x = p_dim, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by dimension",
          .data = dat_qual)
```

### Location

```{r}
my_ggplot(aes(x = location, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by location",
          .data = dat_qual)
```

#### Facet by factor

```{r}
my_ggplot(aes(x = location, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by location, faceted by factor",
          .data = dat_qual) + facet_wrap(vars(factor))
```

#### Facet by covariance shape

```{r}
my_ggplot(aes(x = location, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by location, faceted by covariance shape",
          .data = dat_qual) + 
  facet_wrap(vars(vc))
```

#### Facet by evaluation order

```{r}
my_ggplot(aes(x = location, y = task_marks,
              color = is_training, fill = is_training),
          "Marks by location, faceted by evaluation order",
          .data = dat_qual) + 
  facet_wrap(vars(eval))
```

______

## Speed by factor, order

```{r}
my_ggplot(aes(x = factor, y = max_sec_to_resp,
              color = is_training, fill = is_training),
          "Seconds till last response by factor",
          .data = dat_qual) +
  coord_cartesian(ylim = c(0L, 70L))

my_ggplot(aes(x = eval, y = max_sec_to_resp,
              color = is_training, fill = is_training),
          "Seconds till last response by evaluation order",
          .data = dat_qual) +
  coord_cartesian(ylim = c(0L, 70L))
```

______

## Input and response interactions

```{r}
my_ggplot(aes(x = factor, y = task_input_inter,
              color = is_training, fill = is_training),
          "Number of input interaction (throughness) by factor",
          .data = dat_qual) +
  coord_cartesian(ylim = c(0L, 30L))

my_ggplot(aes(x = factor, y = task_resp_inter,
              color = is_training, fill = is_training),
          "Number of response interaction (inverse confidence) by factor",
          .data = dat_qual)
```


