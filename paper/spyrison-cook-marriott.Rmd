---
documentclass: jdssv
classoption: article
output:
  bookdown::pdf_document2:
    keep_tex: true
    toc: false
    template: template/jdssv_template.tex
    #citation_package: natbib
title:
  formatted: "A study examining the benefit of the user-controlled radial tour for understanding variable contributions to structure visible in linear projections of high-dimensional data"
  plain:     "A study examining the benefit of the user-controlled radial tour for understanding variable contributions to structure visible in linear projections of high-dimensional data"
  short:     "Study on the use of the radial tour for understanding variable importance"
author:
  - name: Nicholas Spyrison
    affiliation: Monash University
    address: |
      | Faculty of Information Technology,
      | Monash University
    email: \email{nicholas.spyrison@monash.edu}
  - name: Dianne Cook
    affiliation: Monash University
    address: |
      | Department of Econometrics & Business Statistics,
      | Monash University
  - name: Kimbal Marriott
    affiliation: Monash University
    address: |
      | Faculty of Information Technology,
      | Monash University
abstract: |
  Radial tours make it possible for a user to interact with a linear projection of data, by rotating a variable in or out. This paper describes a user study examining the use of the radial tour in comparison to two existing methods, principal component analysis and a grand tour, for understanding variable importance. Accuracy and speed is measured for tasks in supervised classification where participants indicate which variables most contribute to the separation between two target clusters. Data was collected from 108 subjects, who completed 648 tasks, using a crowd-sourcing service.  The results suggest that the radial tour tends to increase accuracy. Participants also preferred to use the radial tour for the tasks provided.
##preamble: > ## Moved to /template/jdsssv_template.tex
keywords:
  formatted: [multivariate data, exploratory data analysis, grand tour, manual tour, dimension reduction, linear projections, linear embeddings, "\\proglang{R}"]
  plain:     [multivariate data, exploratory data analysis, grand tour, manual tour, dimension reduction, linear projections, linear embeddings, R]
bibliography: spyrison-cook-marriott.bib
editor_options:
  chunk_output_type: console
urlcolor: blue
linkcolor: red
---
\bibliography{spyrison-cook-marriott}
```{r setup_paper, include=FALSE}
require("knitr")
require("kableExtra")
require("magrittr")
## PDF animations inline:
require("tourr")
require("spinifex")
require("gganimate")
## chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  echo = FALSE,
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  cache = TRUE,
  cache.lazy = FALSE
)
```

# Introduction

<!-- Multivariate spaces and EDA - black-box models-->
Multivariate data underlies most classification problems. Yet exploratory data analysis [EDA, @tukey_exploratory_1977] of such spaces is difficult, increasingly so as dimension increases, and often leads to the consideration of models for these problems being considered to be black-boxes. There is increasing emphasis on the need to provide explainers to improve the interpretability for black-box models [@biecek_dalex_2018; @biecek_explanatory_2021; @lundberg_unified_2017; @ribeiro_why_2016; @wickham_visualizing_2015]. Visualization is an important part of providing interpretations [@anscombe_graphs_1973; @coleman_geometric_1986; @goodman_dirty_2008; @matejka_same_2017]. Tour methods [@lee_review_2021; @cook_grand_2008] provide ways to visualize linear projections of high-dimensional spaces, to obtain an overview of shape (distributions and associations) and anomalies (outliers, clusters). The recently introduced radial tour [@spyrison_spinifex_2020] provides a user controlled manual rotation of variables into and out of a projection, which might especially be useful for studying variable importance.

<!-- Research gap -->
Dimension reduction is commonly used in conjunction with visualization to provide informative low-dimensional summaries of high-dimensional data. There have been several user studies
for dimension reduction comparing across embeddings and display dimensionality [@gracia_new_2016; @wagner_filho_immersive_2018]. There are also empirical metrics and comparisons used to describe non-linear reduction and how well and faithfully they embed the data [@bertini_quality_2011; @liu_visualizing_2017; @sedlmair_empirical_2013; @van_der_maaten_visualizing_2008]. <!-- Add comment about the ensemble graphics applications of multivariate visuals? -->There is an absence of studies comparing techniques for assessing variable importance, particularly, how best to convey information to the viewer.

<!-- Add more gap and lit review here? Wants a better segue. -->
This paper describes a user study conducted to assess the benefit of the radial tour, in comparison with principal component analysis and a grand tour for understanding variable importance. The experiment is a within-participant user study. The type of visualization is the primary factor of the study, corresponding to a null hypothesis that all techniques provide similar ability for the user to determine variable importance. The techniques are compared by having subjects complete several tasks, where accuracy and speed is recorded. 

<!-- structure of the paper -->
The paper is structured as follows. Section \@ref(sec:background) provides the background on the visualization methods being compared. Section \@ref(sec:userstudy) describes the user study, the tasks, evaluation, and measures used. The results of the study are in Section \@ref(sec:results). Conclusions and potential future directions are discussed in Section \@ref(sec:conclusion). The software used for the study is described in Section \@ref(sec:spinifex).


# Background {#sec:background}

## Principal component analysis

Principal component analysis is a good baseline of comparison for linear projections because of its frequent and broad use across disciplines. Principal component analysis [PCA, @pearson_liii._1901] creates new components that are linear combinations of the original variables. The creation of these variables is ordered by decreasing variation which is orthogonally constrained to all previous components. While the full dimensionality is intact, the benefit comes from the ordered nature of the components. The first 2 or 3 components are typically used to approximate the variation multivariate data set, while the rest are discarded.

```{r figFactorPca, echo = F, out.width = '60%', fig.cap = "Scatterplot matrix of the first 3 principal components for an example simulation. This illustrates 3 of the 6 PCA pairings that participants can explore while evaluating a task under PCA."}
if(F)
  file.edit("./paper/R/fig_factor.r")
knitr::include_graphics("./figures/figFactor_pca.pdf")
```


## Scatterplot matrix

<!-- Not Scatterplot matrices, PCP, want mapped to position -->
An extension to showing only a few components is to to show pairs of components as a scatterplot matrix [@chambers_graphical_1983], where permutations of pairs of the components are looked displayed on the upper or lower triangles of the matrix. This is a very dense way to illustrate the data and can obscure important features, by giving a large battery of plots to view simultaneously . Figure \@ref(fig:figFactorPca) shows the orthogonal pairs of the first 3 components as a scatterplot matrix.


## Parallel coordinates plot

Another common way to illustrate this data is with a parallel coordinates plot [@ocagne_coordonnees_1885] (or its polar coordinate variants), plot observations by their quantile values for each variable and connected by lines to the quantile value in subsequent variables. This scales well with dimensionality, but suffers a from couple of issues. The main issue is the loss of mapping multiple variables to graphic position simultaneously as the $x$-axis is being occupied by variables, which Munzner suggests [@munzner_visualization_2014] is the most important visual  channel for human perception. Another issue is that parallel coordinates are asymmetric, as their interpretation is dependent on variable ordering.


## Data visualization tours

<!-- tours intro -->
A data visualization _tour_ uses time to animate local changes in the projection basis. One of the key features of the tour is the object permanence of the data points; that is to say by watching nearby frames one can track the relative changes of observations as the basis moves toward the next target basis. There are various types of tours which are distinguished by the selection or generation of their basis paths [@lee_review_2021; @cook_grand_2008]. To contrast with the discrete orientations of PCA, we compare with continuous changes of linear projection with _grand_ and _radial_ tours.

### Grand tours
<!-- Grand tour -->
In a grand tour [@asimov_grand_1985] the target bases are selected randomly. The grand tour is the first and most widely known tour. It will serve as an intermediate unit of comparison which has continuity of data points in nearby frames along with the radial tour, but lacks the user control enjoyed by PCA and radial tours. This lack of control makes grand tours more of a generalist exploratory tool.


```{r figFactorGrand,echo=F,fig.show='animate',interval=.2,results=F,fig.height=4, fig.cap="Animation of grand tour, a near-continuous change over time as the projection basis approaches the next target basis through orthonormal interpolation. In the grand tour frames are selected randomly. Animating small changes in the basis conveys more information than discrete projections alone. For instance, we can better see how and when the structure of the different clusters separate. The animation is viewable on some pdf viewers including Adobe Acrobat Reader."}
require("gganimate")
require("spinifex")

## Data
.sim_fp <- "./data/EEV_p6_0_1_rep3.rda"
load(.sim_fp, envir = globalenv()) ## Load object `EEV_p6_0_1_rep3`
dat <- EEV_p6_0_1_rep3
clas <- as.factor(attr(dat, "cluster"))

## Tour path
.tpath_fp <- "./data/tpath_p6.rda"
load(.tpath_fp, envir = globalenv()) ## Loads object `tpath_p6`

## Manual spinifex::play_tour_path(render_gganimate) call
.gg <- play_tour_path(tour_path = tpath_p6, data = dat,
                      render_type = render_, 
                      angle = .1,  axes = "left",
                      aes_args = list(color = clas, shape = clas),
                      identity_args = list(size = 1.5, alpha = .7),
                      ggproto = list(theme_spinifex(), 
                                     theme(legend.position = "none"))
)
.gg + gganimate::transition_states(frame, transition_length = 0L)
```


### Radial (manual) tours

<!-- Manual tour -->
The _manual_ tour [@cook_manual_1997] defines its basis path by manipulating the basis contribution of a selected variable. A manipulation dimension is appended onto the projection plane, with a full contribution given to the selected variable. The target bases are then selected based on rotating this newly created manipulation space. The target bases are then similarly orthogonally restrained, data projected and rendered into an animation. In order for the variables to remain independent of each other, the contributions of the other variables must also change, _ie._ dimension space should maintain its orthonormal structure. A key feature of the manual tour is that it affords users a way to control the variable contributions of the next target basis. This means that such manipulations can be selected and queued in advance or select on the spot for human-in-the-loop analysis [@karwowski_international_2006]. However, this navigation is relatively time-consuming due to the huge volume of $p$-space (an aspect of the curse of dimensionality [@bellman_dynamic_1957]) and the abstract method of steering the projection basis. It is advisable to first identify a basis of particular interest and then use a manual tour as a finer, local exploration tool to observe how the contributions of the selected variable do or do not contribute to the feature of interest.

<!-- radial tour variant -->
To simplify the task and keep its duration realistic, we consider a variant of the manual tour, called a _radial_ tour. In a radial tour, the selected variable is allowed to change its magnitude of contribution, but not its angle; it must move along the direction of its original contribution radius. The radial tour benefits from both continuity of the data alongside grand tours, but also allows the user to steer via choosing the variable to rotate.

```{r figFactorRadial,echo=F,fig.show='animate',interval=.1,eval=T,fig.height=4, fig.cap="Animation of the radial, manual tour. Instead of the target bases being selected at random, they instead vary the contribution of a selected variable. In this case variable 1 is rotated fully into and out of the projection plane. The animation is viewable on some pdf viewers including Adobe Acrobat Reader."}
## Interval wants to be half that of the grand tour, has duplicate frames despite the manual tour df's not having frames.

## Data
.sim_fp <- "./data/EEV_p6_0_1_rep3.rda"
load(.sim_fp, envir = globalenv()) ## Load object `EEV_p6_0_1_rep3`
dat <- EEV_p6_0_1_rep3
clas <- as.factor(attr(dat, "cluster"))
bas <- basis_half_circle(dat)

## Manual play_manual_tour(render_gganimate) call
.gg <- play_manual_tour(bas, data = dat, manip_var = 1L, manip_col = "blue",
                        render_type = render_,
                        angle = .2, axes = "left",
                        aes_args = list(color = clas, shape = clas),
                        identity_args = list(size = 1.5, alpha = .7),
                        ggproto = list(theme_spinifex(),
                                       theme(legend.position = "none"))
)

.gg + gganimate::transition_states(frame, transition_length = 0L)
```

The recent implementation of manual tours us the the R package \pkg{spinifex} [@spyrison_spinifex_2020], which facilitates manual tours (and radial variant). It is also compatible with tours made via \pkg{tourr} [@wickham_tourr:_2011], and improves exporting to .gif or .html widget, with recent graphic packages. Now that we have a readily available means to produce various tours, we want to see how they fare against traditional discrete display commonly used with PCA.


# User study {#sec:userstudy}

## Objective {#sec:objective}

We measure the accuracy and speed over 4 dimensions of block parameterizations. The null hypotheses can be stated as:

<!-- Hypothesis: null and alternative -->
$~~~~~H_0: \text{visualization factor does not impact task } \textit{accuracy}, Y_1. \\$
$~~~~~~H_0: \text{visualization factor does not impact task } \textit{speed}, Y_2. \\$

<!-- Rational for standards of comparison -->
PCA will be used as a baseline for comparison as it is the most common linear embedding. The grand tour will act as a secondary control that will help evaluate the benefit of animation, where the object permanence of the data points, but without the ability to influence its path. Lastly, the radial tour should perform best as it benefits both from animation and being able to select an individual variable to change the contribution of.

<!-- Possible expectations -->
<!-- PCA shows orthogonal views of pairs of their components. Whereas, the animated transitions of tours convey a lot of meaningful structural information. The grand tour is good for initial exploration and explores a lot of different angles very quickly. While the manual (and radial) tours are best used as a more local exploration tool, to test the sensitivity of structure.  -->
Then for some subset of tasks, we expect to find that the radial tour performs most accurately with greater control to explore specific variable. Secondly, it may be the case that grand is performs faster than the alternatives with it's absence of inputs all the users attention can focus on interpreting the fixed path. We are less certain with this there is no optimization or functional goal in the grand tour's generation of bases; it is possible that. by chance, the planes completely avoid information needed. However, given that the data dimensionality will be modest, it is at least plausible that grand tour  regularly crosses frames with the correct information to perform the task quickly.


## Task and evaluation {#sec:task}

<!-- Geom, clusters, explicit task -->
The display was a 2D scatterplot with observations supervised. Cluster membership was mapped to shape and color. There were either 3 or 4 clusters each with a constant number of observations within each cluster. Participants were asked to 'check any/all variables that contribute more than average to the cluster separation green circles and orange triangles', which was further explained in the explanatory video as 'mark and all variable that carries more than their fair share of the weight, or 1 quarter in the case of 4 variables'.

<!-- Instruction and video -->
The instructions iterated several times in the video was: 1) Use the input controls to find a frame that contains separation between the clusters of green circles and orange triangles, 2) look at the orientation of the variable contributions in the gray circle, a visual depiction of basis, and 3) select all variables that contribute more than average in the direction of the separation in the scatterplot. Regardless of factor and block values participants were limited to 60 seconds for each evaluation of this task.

<!-- Evaluating measure -->
The evaluation measure of this task was designed with a couple of features in mind: 1) the sum of squares of the individual variable marks should be 1, and 2) symmetric about 0. With these in mind, we define the following measure for evaluating the task.

Let a dataset $\textbf{X}$ be a simulation containing clusters of observations of different distributions. Let $\textbf{X}_k$ be the subset of observations in cluster $k$ containing the $p$ variables.


\begin{align*}
  &\textbf{X}_{n\*p} = (x_1,~...~x_p) \\
  &\textbf{X}_{n_k\*p_k} = (x_1,~...~x_p)
    ~|~ n_k \in [1, n], \text{ is an observation subset of } \textbf{X}
\end{align*}


where


\begin{align*}
  x_{i, j, k} \text{ is scalar; the observation } i \in [1,~...~n],
    \text{ of variable } j \in [1,~...~p], \text{ of cluster } k \in [1,~...~K]
\end{align*}


<!-- W, weights -->
We define weights, $W$ to be a vector explaining the variable-wise difference between 2 clusters. Namely the difference of each variable between clusters, as a proportion of the total difference, less $1/p$, the amount of difference each variable would hold if it were uniformly distributed. <!-- R, participant responses -->Participant responses, $R$, are in the form of a boolean value for each variable, whether or not participant thinks each variable separates the two clusters more than if the difference were uniformly distributed. Then $Y_1$ is a vector of variable marks.


\begin{align*}
W_{j} &=\frac
{(\overline{X_{j=1, k=1}} - \overline{X_{1, k=2}}, ~...~
(\overline{X_{j=p, k=1}} - \overline{X_{j=p, k=2}})}
{\sum_{j=1}^{p}(|\overline{X_{j,k=1}} - \overline{X_{j,k=2}}|)}
- \frac{1}{p} \\
&= (w_1,~...~ w_p) \\
\\
Y_1 &= I(r_j) * sign(w_j) * \sqrt{|w_j|} \\
&= (m_1, ~...~ m_p) \\
\\
\end{align*}


where $I$ is the indicator function. Then the total marks for this task is the sum of this marks vector. We use time till the last response as a secondary dependent variable $Y_2$.

```{r, figBiplotScoring, echo = F, out.width = '100%', fig.cap = "(L), PCA biplot of the components showing the most cluster separation with (R) illustration of the magnitude of cluster separation is for each variable (wide bar) and the weight of the marks given if a variable is selected (red/green line). The horizontal dashed line is 1 / dimensionality, the amount of separation each variable would have if evenly distributed. The weights equal the signed square of the difference between each variable value and the dashed line."}
if(F)
  file.edit("./paper/R/fig_biplot_scoring.r")
knitr::include_graphics("./figures/figBiplotScoring.pdf")
```

<!-- Period order: Training, eval, eval -->
Each of the 3 periods introduced the participant to a new factor, where participants were first able to explore an untimed task with data under the simplest parameterizations. The training allows the participant to become familiar with the inputs and visual specific to the factor. Upon clicking a button to proceed text containing the correct answer displays with visual still intact to explore further. After the training, each participant performed 2 evaluation trials. After 60 seconds the display was turned off, though few participants elapsed this time. These evaluation trials were performed under different parameterizations as explained in section \@ref(sec:blocks).


## Factor application {#sec:factors}

<!-- Background for methodology, application here -->
Section \@ref(sec:background) gives the sources and a description of the visual factors PCA, grand tours, and radial manual tours. The factors are tested within-participant, with each factor being evaluated by each participant. The order that factors are experienced in is controlled with the block assignment as illustrated below in Figure \@ref(fig:figParmeterizationExample). Below we cover the visual design standardization, as well the unique input and display within each factor.

<!-- aesthetic standardization -->
The visualization methods were standardized wherever possible. each factor was shown as a biplot, with variable contributions displayed on a unit circle. All aesthetic values (colors, shapes, sizes, absence of legend, and absence axis titles) were held consistent. Variable contributions were always shown left of the scatterplot embeddings with their aesthetic values consistent as well. What did vary between factors were their inputs which caused a discrete jump to another pair or principal components, were absent for the grand tour with target bases to animate through selected at random, or for the radial tour which variable should have its contribution animated.

<!-- PCA -->
PCA inputs allowed for users to select between the top 4 principal components for both the x and y-axis regardless of the data dimensionality (either 4 or 6). <!-- Grand tours -->There was no user input for the grand tour, users were instead shown a 15-second animation of the same randomly selected path. Users were able to view the same clip up to 4 times within the time limit. <!-- Radial tours -->Radial tours were also displayed at 5 frames per second within the interpolation step size of 0.1 radians. Users were able to swap between variables, upon which the display would change the start of radially increasing the contribution of the selected variable till it was full, zeroed, and then back to its initial contribution. The complete animation of any variable takes about 20 seconds and is almost fully in the projection frame at around 6 seconds. The starting basis of each is initialized to a half-clock design, where the variables were evenly distributed in half of the circle which is then orthonormalized. This design was created to be variable agnostic while maximizing the independence of the variables.


## Blocks and parameterization {#sec:blocks}

<!-- volume of parameter space -->
In addition to visual factor, we vary the data across 3 aspects: 1) The location of the difference between clusters, by mixing a signal and a noise variable at different ratios, we control which variables contain cluster separation, 2) the shapes of the clusters, by changing the variance-covariance matrices, and 3) the dimensionality of the data.

<!-- Dimensionality -->
Dimensionality is tested at 2 modest levels, namely, in 4 dimensions containing 3 clusters and 6 dimensions with 4 clusters. Each cluster samples 140 observations. Each dimension is originally distributed as $\mathcal{N}(2 * I(signal), 1)~|~\text{covariance}~\Sigma$, before applying location mixing and standardizing by standard deviation). Signal variables have a correlation of 0.9 when they have equal orientation and -0.9 when their orientations vary. Noise variables were restricted to 0 correlation. Within each factor-period dimension is fixed with increasing difficulty, 4 then 6.

<!-- Shape, vc matrix -->
For choosing the shape of the clusters we follow the convention given by Scrucca, [@scrucca_mclust_2016] who named and categorize 14 variants of model families containing 3 clusters. The name of the model family is the abbreviation of its respective volume, shape, and orientation, which are either equal or vary. We use the models EEE, EEV, and EVV, the latter is further modified by moving 4 fifths of the data out in a "V" or banana-like shape. Figure \@ref(fig:figModelFamilies) shows the principal component biplot of the 3 model variants applied here. The training always uses 4 dimensions, while the 2 evaluations always contain 4 and 6 dimensions in the order of increasing difficulty. The evaluation periods use EEE, EEV, and EVV-banana respectively in increasing order of difficulty.

```{r figModelFamilies, echo = F, out.width = '100%', fig.cap = "Ellipses of the isodensity of the model families used. Family labels are are the abbreviation for the clusters volume, shape, and orientation respectively, which are either equal or vary. We further change the EVV model by shifting fifths of the data in banana or chevron arrow shape."}
if(F)
  file.edit("./paper/R/fig_model_families.r")
knitr::include_graphics("./figures/figModelFam.pdf")
```

<!-- Location mixing -->
The separation of any pair of clusters is currently contained fully within a single variable at this point. To test the sensitivity to this we mix a noise variable with the signal-containing variable such that the difference in the clusters is mixed at the following percentages: 0/100 (not mixed), 33/66, 50/50 (evenly mixed). The training always uses 4 dimensions, while the 2 evaluations always contain 4 and 6 dimensions in the order of increasing difficulty. The training data does not mix signal Location mixing within an evaluation period is held constant and rotated through the 6 permutations of their order. Randomizing the order of the location mixing is controlled by iterating once after each of the 6-factor order permutations are evaluated. This is illustrated in Figure \@ref(fig:figParmeterizationExample).

```{r figParmeterizationExample, echo = F, out.width = '100%', fig.cap = "Illustration of how a hypothetical participant 63 is assigned factor and block parameterizations. Each of the 6-factor order permutations is exhausted before iterating to the next permutation of location order."}
knitr::include_graphics("./figures/figParmeterizationExample.png") ## this is .pttx screen cap, .png ok.
```

<!-- Eval evaluation, selection of size -->
With this setup, we test the parameter space $dimension \in (4,~ 6),~shape \in(EEE,~ EEV,~ EVV-banana),~location ~\in~ (0/100,~ 33/66,~ 50/50)$ percent noise/signal mixing to evaluate the graphic display across the $factors~\in~(PCA,~grand,~radial).$ As we iterate through the possible permutations of these factors and location we perform an even evaluation of the full parameter space every 36 participants. Via pilot studies, we estimate that 3 even block evaluations should be sufficient to identify the difference between the factors; we targeted for $N = 108$ participants.

<!-- Learning effect -->
In addition to the explicitly controlled block parameters, we will also be discussing each participant's evaluation order regardless of factor or location experiences. This will expose a learning effect from the repetition of being exposed to this data or problem. Keep in mind that the shape and location are always experienced in order of increasing difficulty.



## Sampling population {#sec:population}

We recruited $N = 108$ via prolific.co [@palan_prolific_2018]. We make the assumption that interpretation of biplot displays used will not be commonly used for consumption by the general population and apply a single filter on education; that participants have completed at least an undergraduate degree (some 58,700 of the 150,400 users at the time). There is also the implicit filter that Prolific participants must be at least 18 years of age. Participants were compensated for their time at \pounds 7.50 per hour, whereas the mean duration of the survey was about 16 minutes. We can't preclude previous knowledge or experience with the factors, but instead, try to control for this in the user study. Figure \@ref(fig:figSurveyDemographics) shows distributions of age and preferred pronouns of the participants that completed the post-study survey who are relatively young and well educated.

```{r, figSurveyDemographics, echo = F, out.width = '100%', fig.cap = "Heatmaps of participant demographics, counts of age group by completed education as faceted across preferred pronoun. Our sample tended to be between 18 and 35 years of age with an undergraduate or graduate degree."}
if(F) ## Creation and saving figure at:
  file.edit("./paper/R/fig_population_demographics.r")
knitr::include_graphics("./figures/figSurveyDemographics.pdf")
```

## Evenness of block evaluation

From pilot studies through a sample of convenience (consisting of information technology and statistics Ph.D. students attending Monash University), we predict that we wanted 3 even block evaluations to support differences in our factor and block parameterizations. Given that factor and location, each have 6 permutations we targeted $N = 108 = 3 * (6 * 6)$ evaluations before data were collected. In data collection, we experienced several adverse conditions, primarily: limited control of application server network configuration, throughput thresholds on data read/write API, and repeat attempts from users when experiencing disconnects. To mitigate this we over-collect survey trials, exclude all partial trials and remove the oldest attempts (mostly likely to experience adverse network conditions) from over evaluated permutations until we have our desired evaluations under each permutation.

## Data capture and processing

Data was recorded by a \pkg{shiny} application and was written to a Google Sheet after each study period. Time is measured as the CPU time of the server. This could be made slightly more accurate by calculating the difference of system time at these points. Because the frequency of the participants from prolific was more then the server would handle, participants were manually accepted one or two at a time. We collected participants till we had 3 evaluations of each parameter permutation. The processing steps were fairly minimal after formatting to tidy format and decoding values to a more human-readable state. After formatting, a flag is added to indicate if the survey had data from all 3 periods. We remove all partial studies and keep only the last 3 complete studies within each block parameterization, the studies which should have experienced the least adverse network conditions. This brings us to the 108 studies described in the paper, from which models and aggregation tables were built. The post-study surveys were similarly decoded to human-readable tidy format and then filtered to include only those 84 surveys that were associated with the final 108 studies. 

The code, response files, their analyses, and the study application are publicly available at on GitHub \url{https://github.com/nspyrison/spinifex_study}.


# Results {#sec:results}

To recap, the primary response variable is task marks as defined in section \@ref(sec:task), and the log of response time will be used as a secondary response variable. We have 2 primary data sets; the user study evaluations and post-study survey. The former is contains the 108 trials with explanatory variables: _factor_, _location_ of the cluster separation signal, the _shape_ of variance-covariance matrix, and the _dim_-ensionality of the data. Block parameterization and randomization were discussed in section \@ref(sec:blocks). The survey was completed for 84 of these 108 trials and contains demographic information (preferred pronoun, age, and education), and subjective measures for each of the factors (*preference*, *familiarity*, *ease of use*, and *confidence*).

Below we look at the marginal performance of the block parameters and survey responses. After that, we build a battery of regression models to explore the variables and their interactions. Lastly, we look at the subjective measures between the factors.


## Random effect regression against marks

<!-- Note that we list the measures at the top of the Results section -->
<!-- Introduce regression model explaining marks, and the random effect term  -->
To more thoroughly examine explanatory variables, we regress against marks. All models have a random effect term on the participant, which captures the effect of the individual participant. After we look at models of the block parameters we extend to compare against survey variables. Last, we compare how adding a random effect for data and regressing against time till last response fares against benchmark models. The matrices for models with more than a few terms quickly become rank deficient; there is not enough information in the data to explain all of the effect terms. In which case the least impactful terms are dropped.

<!-- Building a battery of models -->
In building a set of models to test we include all single term models, a model with all independent terms. We also include an interaction term of factor by location, allowing for the slope of each location to change across each level of the factor, which is feasible. For comparison, an overly complex model with many interaction terms is included.

<!-- _Context: 648 task evaluations from 108 studies; we regress against marks using block parameters_ -->
<!-- 1) Eval parameters -->
$$
\begin{array}{ll}
\textbf{Fixed effects:}          &\textbf{Full model:} \\
\alpha                           &\widehat{Y_1} = \mu + \alpha_i + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha + \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i + \beta_j + \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma * \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k * \delta_l + \textbf{Z} + \textbf{W} + \epsilon
\end{array}
$$
$$
\begin{array}{ll}
\text{where } &\mu \text{ is the intercept of the model including the mean of random effect} \\
&\epsilon   \sim \mathcal{N}(0,~\sigma), \text{ the error of the model} \\
&\textbf{Z} \sim \mathcal{N}(0,~\tau), \text{ the random effect of participant} \\
&\textbf{W} \sim \mathcal{N}(0,~\upsilon), \text{ the random effect of simulation} \\
&\alpha_i \text{, fixed term for factor}~|~i\in (\text{pca, grand, radial}) \\
&\beta_j  \text{, fixed term for location}~|~j\in (\text{0\_1, 33\_66, 50\_50}) \text{ \% noise/signal mixing} \\
&\gamma_k \text{, fixed term for shape}~|~k\in (\text{EEE, EEV, EVV banana}) \text{ model shapes} \\
&\delta_l \text{, fixed term for dim}~|~l\in (\text{4 variables \& 3 cluster, 6 variables \& 4 clusters}) \\
\end{array}
$$

```{r marksCompTbl, fig.cap = "Use the caption arg in kable(), not this."}
if(F) ## Creation and saving at:
  file.edit("./apps_supplementary/v4_prolifico_100/_analysis.rmd")
model_comp_tbl_ls <- readRDS("./figures/modelCompLs.rds")

model_comp_tbl_ls[[1]]$`Fixed effects` <- model_comp_tbl_ls[[2]]$`Fixed effects` <-
  c("a", "a+b+c+d", "a*b+c+d", "a*b*c+d", "a*b*c*d")
## 1) Eval parameters
kableExtra::kbl(
  model_comp_tbl_ls[[1]], "latex", align = c("l", rep("l", 2), rep("c", 5)),
  booktabs = TRUE, linesep = "", caption = "Model comparison of our random effect models regressing marks. Each model includes a random effect term of the participant, which explains the individual's influence on their marks. Complex models perform better in terms of R2 and RMSE, yet AIC and BIC penalize their large number of fixed effects in favor of the much simpler model containing only factor.") %>%
  kableExtra::column_spec(column = 1, width = "1.5cm") %>%
  column_spec(column = c(2:3), width = "1cm") %>%
  column_spec(column = c(4:8), width = "1.5cm") %>%
  kable_styling(bootstrap_options = "striped", font_size = 10)
```

<!-- Select model ABcd -->

<!-- Coefficients of the selected model, ABcd -->
```{r marksCoefTbl, fig.cap = "Use the caption arg in kable(), not this."}
if(F) ## Creation and saving at:
  file.edit("./apps_supplementary/v4_prolifico_100/_analysis.rmd")
coef_ls <- readRDS("./figures/modelCoefLs.rds")

## Formatting
rownames(coef_ls[[1]]) <- rownames(coef_ls[[2]]) <-
  c("(Intercept)" ,"fct=grand" ,"fct=radial"
    ,"loc=33_66"  ,"loc=50_50" ,"shp=EEV"
    ,"shp=ban"    ,"dim=6"     ,"fct=grand:loc=33_66"
    ,"fct=radial:loc=33_66"    ,"fct=grand:loc=50_50", "fct=radial:loc=50_50")

## For grouping rows see:
if(F)
  browseURL("https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html#Group_rows_via_labeling")
kableExtra::kbl(coef_ls[[1]], format = "latex", booktabs = TRUE, linesep = "",
                caption = "The task accuracy model coefficients for $\\widehat{Y_1} = \\alpha * \\beta + \\gamma + \\delta$, with factor=pca, location=0/100, and shape=EEE held as baselines. Factor being radial is the fixed term with the strongest evidence in support of the hypothesis. When crossing factor with location radial performs worse with 33/66 percent mixing relative to the PCA with no mixing. The model fit is based on the 648 evaluations by the 108 participants.") %>%
  kableExtra::column_spec(column = 1, width = "5cm") %>%
  kableExtra::column_spec(column = 3, width = "1.6cm") %>%
  kableExtra::column_spec(column = c(2, 4:6), width = "1.4cm") %>%
  kableExtra::pack_rows("factor", 2, 3) %>%
  pack_rows("fixed effects", 4, 8) %>%
  pack_rows("interactions", 9, 12) %>%
  kable_styling(bootstrap_options = "striped", font_size = 10)
```

<!-- Random effects vs Mean Mark CI by participant and sim -->
Residual plots have no noticeable non-linear trends and contain striped patterns as an artifact from regressing on discrete variables. Figure \@ref(fig:figEffectRange) illustrates (T) the effect size of the random terms participant and simulation, or more accurately, the 95\% CI from Gelman simulation of their posterior distribution. The effect size of the participant is much larger than simulation. The most extreme participants are statistically significant at $\alpha = .95$, while none of the simulation effects significantly deviate from the null of having no effect size on the marks. In comparison, (B) 95\% confidence intervals of the mean marks for participation and simulation respectively.

```{r figEffectRange, out.width="100%", fig.show='asis', fig.cap="(T) Estimated effect ranges of the random effect terms participant and data simulation of the $\\alpha * \\beta + \\gamma + \\delta$ model. Confidence intervals are created with Gelman simulation on the effect posterior distributions. The effect size of the participant is relatively large, with several significant extrema. None of the simulations deviate significantly. (B) The ordered distributions of the CI of mean marks follow the same general pattern and give the additional context of how much variation is in the data, an upper limit to the effect range. The effect ranges capture about 2/3 of the range of the data without the model. All intervals for $\\alpha = .95$ confidence."}
if(F) ## Creation and saving figure at:
  file.edit("./apps_supplementary/v4_prolifico_100/_analysis.rmd")
knitr::include_graphics("./figures/figEffectRange.pdf")
```

<!-- Conditional effects of variables -->
We also want to visually explore the conditional variables in the model. Figure \@ref(fig:figMarksABcd) explores violin plots of marks by factor while faceting on the location (vertical) and shape (horizontal). Radial tends to increase the marks received, and especially so when there is no signal/noise mixing. 

```{r, figMarksABcd, echo = F, out.width = '100%', fig.cap = "Violin plots of terms of the model $\\widehat{Y_1} = \\alpha * \\beta + \\gamma + \\delta$. Overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests suitable for handling discrete data. Participants are more confident and find the radial easier to use relative to the grand tour. Participants claim low familiarity as we expect from crowdsourced participants. Radial is more preferred compared with either alternative for this task."}
if(F) ## Creation and saving figure at:
  file.edit("./apps_supplementary/v4_prolifico_100/_analysis.rmd")
knitr::include_graphics("./figures/figMarksABcd.pdf")
```

## Time regressing models

As a secondary explanatory variable, we also want to look at time. First, we take the log transformation of time as it is right-skewed. Now we repeat the same modeling procedure, namely: 1) build a battery of all additive and multiplicative models. 2) Compare their performance, reporting some top performers. 3) Select a model to examine its coefficients.


```{r timeCompTbl, fig.cap = "Use the caption arg in kable(), not this."}
## 1) Eval parameters
kableExtra::kbl(
  model_comp_tbl_ls[[2]], "latex", align = c("l", rep("l", 2), rep("c", 5)), 
  booktabs = TRUE, linesep = "", caption = "Model comparisons for log(time) models, $\\widehat{Y_2}$ random effect models, where each model includes random effect terms for participants and simulations. We see the same trade-off where the simplest factor model is perferred by AIC/BIC, while R2 and RMSE perform best with the full multiplicative model. We again select the model $\\alpha * \\beta + \\gamma + \\delta$ to explore further as it has relatively high marginal $R^2$ while having much less complexity than the full model.") %>%
  kableExtra::column_spec(column = 1, width = "1.5cm") %>%
  column_spec(column = c(2:3), width = "1cm") %>%
  column_spec(column = c(4:8), width = "1.5cm") %>%
  kable_styling(bootstrap_options = "striped", font_size = 10)
```

```{r timeCoefTbl, fig.cap = "Use the caption arg in kable(), not this."}
kableExtra::kbl(coef_ls[[2]], booktabs = TRUE, linesep = "", format = "latex",
                caption = "The log(time) model coeffients for $Y_2 = \\alpha * \\beta + \\gamma + \\delta$, with factor=pca, location=0/100, and shape=EEE held as baselines. Location=50/50 is the fixed term with the strongest evidence and takes less time. In contrast, the interaction term location=50/50:shape=EEV has the most evidence and takes much longer on average.") %>%
  kableExtra::column_spec(column = 1, width = "5cm") %>%
  kableExtra::column_spec(column = 3, width = "1.6cm") %>%
  kableExtra::column_spec(column = c(2, 4:6), width = "1.4cm") %>%
  kableExtra::pack_rows("factor", 2, 3) %>%
  pack_rows("fixed effects", 4, 8) %>%
  pack_rows("interactions", 9, 12) %>%
  kable_styling(bootstrap_options = "striped", font_size = 10)
```

```{r figTeffectRange, out.width="100%", fig.show='asis', fig.cap='(T) The effect ranges of Gelman resimulation on posterior distributions. These show the magnitude and distributions of particular participants and simulations. Simulation has a relatively small effect on response time. (B) Confidence intervals for mean log(time) by participant and simulation. The marginal density shows that the response times are left-skewed after log transformation. Interpreting back to linear time there is quite the spread of response times: $e^{1}=2.7$, $e^{2.75}=15.6$, $e^{3.75}=42.5$ seconds. Considering simulations, on the right, the bottom has a large variation of time, relative to the effect ranges which means that the variation is explained in the terms of the model and not by the simulation itself.'}
if(F) ## Creation and saving figure at:
  file.edit("./apps_supplementary/v4_prolifico_100/_analysis.rmd")
knitr::include_graphics("./figures/figTeffectRange.pdf")
```

## Subjective measures

The 84 evaluations of the post-study survey also collect 4 subjective measures for each factor. Figure \@ref(fig:figSubjectiveMeasures) shows the Likert plots, or stacked percentage bar plots, alongside violin plots with the same non-parametric, ranked sum tests previously used. Participants preferred to use radial for this task. Participants were also more confident of their answers and found radial tours easier to use compared with the grand tour. All factors have reportedly low familiarity something we expect from crowdsourced participants.

```{r figSubjectiveMeasures, out.width="100%", fig.show='asis', fig.cap='The subjective measures of the 84 responses of the post-study survey, 5 discrete Likert scale levels of aggrement. (L) Likert plots (stacked percent bar plots) with (R) violin plots of the same measures. Violin plots are overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests.'}
if(F)
  browseURL("https://bookdown.org/Rmadillo/likert/is-there-a-significant-difference.html#permutation-mann-whitney-tests")

if(F) ## Creation and saving figure at:
  file.edit("./paper/R/fig_population_demographics.r")
knitr::include_graphics("./figures/figSubjectiveMeasures_w.violin_hori.pdf")
```


# Conclusion {#sec:conclusion}

Above we discussed an $n=108$, with-in participant user study comparing the efficacy of 3 linear projection techniques. The participants performed a supervised cluster task, specifically the identification of which variables contribute to the separation between 2 target clusters. This was evaluated evenly over 4 block parameterizations. In summary, we find that radial tours increases accuracy while the grand tour decreases the time it takes to perform this task. These effects are large relative to the other block parameterizations, but smaller than the random effect of the participant. Radial tour was subjectively most preferred, lead to more confidence in answers, and is easier to use than alternatives.

<!-- Discussion and goint further -->
There are several ways that this study could be extended. In addition to expanding the support of the block parameterizations, more interesting directions include: type of the task, visualizations used, and experience level of the target population. It is difficult to achieve good coverage given the number of possible permutations. Be sure to step back and plan what the target support of your block parameters. Keep in mind the volume and quality of responses from participants especially when crowdsourcing. These planning steps are useful for navigating when the complexity of the application details.


# Accompanying tool: radial manual tour application {#sec:spinifex}

To accompany this study we have produced a more general use tool to perform such exploratory analysis of high dimensional data. The \proglang{R} package, \pkg{spinifex}, [@spyrison_spinifex_2020] contains a free, open-source \pkg{shiny} [@chang_shiny_2020] application. The application allows users to upload, process, and interactively explore their data. Users can quickly traverse global and local extrema and then explore the nearby space with the radial tour as similarly applied in the user study. Limited implementations of grand, little, and local tours are also made available. Data can be imported in .csv and .rda format, and projections or animations can be saved as .png, .gif, and .csv formats where applicable. Run the following \proglang{R} code for help getting started.

```{r getting_started, eval=FALSE, echo=TRUE}
install.packages("spinifex", dependencies = TRUE)
spinifex::run_app("intro")
```

# Acknowledgments {#sec:acknowledgments}

This research was supported by an Australian Government Research Training Program (RTP) Scholarship. This article was created in \proglang{R} [@r_core_team_r:_2020] and \pkg{rmarkdown} [@xie_r_2018]. Visuals were prepared with \pkg{spinifex}. All packages used are available from the Comprehensive \proglang{R} Archive Network (CRAN) at \url{https://CRAN.R-project.org/}. The source files for this article, application, data, and analysis can be found at \url{https://github.com/nspyrison/spinifex_study/}. The source code for the \pkg{spinifex} package and accompanying shiny application can be found at \url{https://github.com/nspyrison/spinifex/}.

If you are looking for a vignette to reproduce animations in for a pdf document from R with base or \pkg{gganimate} graphics, see the reproducible example: \url{https://github.com/nspyrison/spinifex_study/blob/master/zDevExamples/animated_pdf.rmd}.

# References {#sec:bib}
