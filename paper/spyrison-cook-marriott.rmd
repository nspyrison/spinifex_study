---
title: A Study on the Benefit of a User-Controlled Radial Tour for Variable Importance for Structure in High-Dimensional Data
affiliation:
  ## use one only of the following #institution-columnar removed.
  author-columnar: true ## one column per author
  #wide: true            ## one column wide author/affiliation fields
  institution:
    - name: Monash University
      location: Australia
      mark: 1
      author:
        - name: Nicholas Spyrison
          email: |
            | nicholas.spyrison@monash.edu
            | ORCiD: 0000-0002-8417-0212
        - name: Dianne Cook
          email: |
            | dicook@monash.edu
            | ORCiD: 0000-0002-3813-7155
        - name: Kim Marriott
          email: |
            | kim.marriott@monash.edu
            | ORCiD: 0000-0002-9813-0377
keywords: ["Linear Dimension Reduction", "Visual Analytics", "Grand Tour", "Data Science", "Machine Learning", "Data Clustering", "Explainable Artificial Intelligence"]
abstract: |
  Principal component analysis is a long-standing go-to method for exploring multivariate data. The principal components are linear combinations of the original variables, ordered by descending variance. The first few components typically provide a good visual summary of the data. _Tours_ also make linear projections of the original variables but offer many different views, like examining the data from different directions. The grand tour shows a smooth sequence of projections as an animation following interpolations between random target bases. The manual radial tour rotates the selected variable's contribution into and out of a projection. This allows the importance of the variable to structure in the projection to be assessed. This work describes a within-participants user study evaluating the radial tour's efficacy compared with principal component analysis and the grand tour. A supervised classification task is assigned to participants who evaluate variable attribution of the separation between two classes. Their accuracy in assigning the variable importance is measured across various factors. Data were collected from 108 crowdsourced participants, who performed two trials with each visual for 648 trials in total. Mixed model regression finds strong evidence that the radial tour results in a large increase in accuracy over the alternatives. Participants also reported a preference for the radial tour in comparison to the other two methods.
with_amsmath: true
bibliography: spyrison-cook-marriott.bib
## cls defaults to 10pt, author guide saya 11pt for 2 col, 12pt for single col.
classoption: 11pt,twocolumn
#classoption: 12pt,necolumn
output:
  bookdown::pdf_book:
    keep_tex: true
    base_format: rticles::ieee_article
    # includes:
    #   in_header: "preamble.tex"
editor_options:
  chunk_output_type: console
header-includes:
    - \usepackage{amsmath}
    - \usepackage{hyperref}
    - \hypersetup{colorlinks = true, linkcolor = red, urlcolor = blue}
    - \usepackage{mathtools} ##%% for \shortintertext{} in aligned
    - \usepackage{graphicx}
    - \PassOptionsToPackage{hyphens}{url}\usepackage{hyperref} ##%% Trying to wrap and format urls correctly
---
```{r setup, include=FALSE}
kable_font_size <- 8
require("knitr")
require("kableExtra")
require("magrittr")

## chunk options
knitr::opts_chunk$set(
  fig.align  = "center",
  echo       = FALSE,
  collapse   = TRUE,
  message    = FALSE,
  warning    = FALSE,
  error      = FALSE,
  cache      = FALSE,
  cache.lazy = FALSE
)
```

# Introduction

<!-- Context and motivation; multivariate spaces and EDA -->
Despite decades of research, multivariate data continues to provide fascinating challenges for visualization. Data visualization is important because it is a key element of exploratory data analysis [EDA, @tukey_exploratory_1977] for assessing model assumptions and as a cross-check on numerical summarization [@anscombe_graphs_1973; @matejka_same_2017; @yanai_hypothesis_2020]. One of the challenges is determining whether a new technique yields a better perception of information than current practices for multivariate data.

<!-- Common options; few principal components -->
Dimension reduction is commonly used with visualization to provide informative low-dimensional summaries of quantitative multivariate data. Principal component analysis (PCA) [@pearson_liii._1901] is one of the first methods ever developed, and it remains very popular. Visualization of PCA is typically in the form of static scatterplots of a few leading components. When accompanied by a representation of the linear combination of the original variables (magnitude and angles of the variable contributions are inscribed on a unit circle), they are called biplots [@gabriel_biplot_1971].

Dynamic visualizations called _tours_ [@asimov_grand_1985], animate through a sequence of linear projections (orthonormal bases or frames). Instead of a static view, tours provide a smoothly changing view by interpolating between frames. There are various types of tours distinguished by how the paths are generated. Asimov originally animated between randomly selected bases in the _grand_ tour. The _manual_ tour [@cook_manual_1997] allows for user control over the basis changes. A selected variable (or component) can be rotated into or out of view or to a particular value. The _radial tour_ [@spyrison_spinifex_2020] is a variant of the manual tour that fixes the contribution angle and changes the magnitude along the radius. The permanence of the data points from frame to frame holds information between intermediate interpolated frames, and the user control of the basis could plausibly lead to more information being perceived than a static display. This is a hypothesis that a user study could assess.

<!-- Nonlinear DR quality review -->
Empirical studies have rarely assessed tours. An exception is @nelson_xgobi_1999, who compares scatterplots of grand tours on 2D monitors with 3D (stereoscopic, not head-mounted) over $n=15$ participants. Participants perform cluster detection, dimensionality estimation, and radial sparseness tasks on six-dimensional data. They find that stereoscopic 3D leads to more accuracy in the cluster identification, though the time to interact with the display was much higher in the 3D environment. In this work, we extend the evaluation of tours which compares the radial tour as benchmarked against the grand tour and discrete pairs of principal components.

<!-- Contribution statement -->
The contribution of this paper is an empirical user study comparing the radial tour against PCA and the grand tour for assessing variable attribution on clustered data. This is the first empirical evaluation of the radial or manual tour. We discuss how this fits with other multivariate data visualization techniques and coordinated views of linear projections.

<!-- Overview of the study -->
We are particularly interested in assessing the effectiveness of the new radial tour relative to common practice with PCA and grand tour.<!-- thesis_ns introduction --> The user influence over a basis, uniquely available in the radial tour, is crucial to testing variable sensitivity to the structure visible in projection. If the contribution of a variable is reduced and the feature disappears, then we say that the variable was sensitive to that structure. For example, Figure \@ref(fig:figClSep) shows two frames of simulated data. Panel (a) has identified separation between the two clusters. The contributions in panel (b) show no such cluster separation. The former has a large contribution of V2 in the direction of separation, while it is negligible in the right frame. Because of this, we say that V2 is sensitive to the separation of the clusters.

```{r figClSep, echo = F, out.width = "100%", fig.env = "figure*", fig.cap = "Illustration of cluster separation affected by variable importance. Panel (a) is a projection mostly of V2 and V3, and the separation between clusters is in the direction of V2, not V3. This suggests V2 is important for clustering, but V3 is not. Panel (b) shows a projection of mostly V3 and V4, with no contribution from V2 and little from V3. That there is no separation between the clusters indicates that V3 and V4 are not important."}
if(F)
  file.edit("./paper/R/fig_biplot_scoring.r")
knitr::include_graphics("./figures/figClSep.pdf")
```

<!--- Black-box models-->
Variable sensitivity is important for the interpretation of machine learning models. They are the magnitude and direction of contribution to the model. It is important that we maintain the interpretability of models as their terms becoming increasingly complex. Exploratory Artificial Intelligence [XAI, @adadi_peeking_2018; @arrieta_explainable_2020] is an emerging field that extends the interpretability of such black-box models. Multivariate data visualization is essential for exploring features spaces and communicating interpretations of models [@biecek_dalex_2018; @biecek_explanatory_2021; @wickham_visualizing_2015].

<!-- Structure of the paper -->
The paper is structured as follows. Section \@ref(sec:relatedwork) provides background on standard visualization methods and linear dimension reduction techniques. Section \@ref(sec:userstudy) describes the experimental factors, task, and accuracy measure used. The results of the study are discussed in Section \@ref(sec:results). Conclusions and potential future directions are discussed in Section \@ref(sec:conclusion). More results, participant demographics, and analysis of the response time are available in the appendix, Section \@ref(sec:appendix).


# Related work {#sec:relatedwork}

Consider the data to be a matrix of $n$ observations (rows) and $p$ variables (columns), denoted as $X_{n \times p}$.

## Orthogonal multivariate visualization

@grinstein_high-dimensional_2002 illustrate many multivariate visualization methods. In particular, this work shows examples of actual visuals. @liu_visualizing_2017 give a good classification and taxonomy of such methods. The content below focuses on the most common visuals that use the full data space before discussing linear combinations of those variables in projections.


### Scatterplot matrix

One could consider looking at $p$ histograms or univariate densities. Doing so will miss features in two or more dimensions. Figure \@ref(fig:figFactorPca) shows a scatterplot matrix [@chambers_graphical_1983] of the four principal components of simulated data. Such displays do not scale well with dimensions because each plot would get less and less space. Scatterplot matrices can only display information in two orthogonal dimensions, so features in three dimensions may not be fully resolved.


### Parallel coordinates plot

<!-- PCP -->
Another common way to display multivariate data is with a parallel coordinates plot [@ocagne_coordonnees_1885]. Parallel coordinates plots scale well with dimensions but poorly with observations as the lines overcrowd the display. <!-- issues with obs-based visuals --> Parallel coordinate plots are asymmetric across variable ordering. In that, shuffling the order of the variable can lead to different conclusions. Another shortcoming is the graphical channel used to convey information. @munzner_visualization_2014 suggests that position is the visual channel that is most perceptible to humans. In the case of parallel coordinates plots, the horizontal axes span variables rather than the values of one variable, causing the loss of a display dimension to be used by our most perceptible visual channel.


## Multivariate projections

<!-- Turning to Linear DR -->
At some point, visualization will be forced to turn to dimension reduction to scale better with the dimensionality of the data. Below we introduce linear projections and the common principal component analysis. Then we touch on nonlinear projections and exclude them from consideration.


### Linear

Let data, $X$, contain $n$ observations of $p$ variables. A linear projection maps a higher $p$-dimensional space onto a smaller $d$-space with an affine mapping (where parallel lines stay parallel). A projection, $Y$, is the resulting space of the data multiplied by a _basis_, $A$, such that $Y_{n \times d} = X_{n \times p} \times A_{p \times d}$. This is essentially a reorientation of the original variable. This intuition is conveyed by thinking of a shadow as a 2D projection of a 3D object. Rotating the object changes the shadow it casts and, correspondingly, the basis that maps the reorientation of the object.


### Principal component analysis

PCA is a good baseline of comparison for linear projections because of its frequent and broad use across disciplines. PCA [@pearson_liii._1901] defines new components, linear combinations of the original variables, ordered by decreasing variation through the help of eigenvalue matrix decomposition. While the resulting dimensionality is the same size, the benefit comes from the ordered nature of the components. The data can be said to be approximated by the first several components. The exact number is subjectively selected given the variance contained in each component, typically guided from a scree plot [@cattell_scree_1966]. Features with sizable signal regularly appear in the leading components that commonly approximate data. However, this is not always the case and component spaces should be fully explored to look for signal in components with less variation. This is especially true for cluster structure [@donnell1994].

```{r figFactorPca, echo = F, out.width = "100%", fig.cap = "Scatterplot matrix of the first four principal components of 6D simulated data containing four classes. The separation between classes is primarily in PC1 and PC4. This is not uncommon because PCA is summarizing variance, not cluster structure."}
if(F)
  file.edit("./paper/R/fig_pca_splom.r")
knitr::include_graphics("./figures/fig_pca_splom.pdf")
```


### Nonlinear

Nonlinear transformations bend and distort spaces that are not entirely accurate or faithful to the original variable space. There are various quality metrics, such as Trustworthiness, Continuity, Normalized stress, and Average local error, have been introduced to describe the distortion of the space [@espadoto_toward_2021; @gracia_new_2016]. Unfortunately, these distortions are hard to visualize and comprehend, effectively breaking the variable interpretability of the resulting space. The intuition of this can be demonstrated with map projections. @snyder_map_1987 lists over 200 different projections that distort the surface of the earth to display as a 2D map, each with unique properties and use cases.

Because of the difficulty of interpreting the distortions of nonlinear spaces and the added subjectivity of hyperparameter selection, we exclude nonlinear techniques and instead, decide to compare three linear techniques.


## Tours, animated linear projections {#sec:tours}

<!-- tours intro -->
A _tour_ animates through many linear projections. One of the insightful features of the tour is the permanence of the data points; one can track the relative changes of observations as the basis changes, as opposed to discretely jumping to an orthogonal view angle with no intermediate information. Types of tours are distinguished by the generation of their basis paths [@lee_state_2021; @cook_grand_2008]. To contrast with the discrete orientations of PCA, we compare continuous linear projection changes with grand and radial tours.


### Grand tours

<!-- Grand tour -->
Target bases are selected randomly in a grand tour [@asimov_grand_1985]. These target bases are then geodesically interpolated for a smooth, continuous path. The grand tour is the first and most widely known tour. The random selection of target bases makes it a general unguided exploratory tool. The grand tour will make a good comparison that has continuity of data points similar to the radial tour but lacks the user control enjoyed by PCA and radial tours.


### Manual and radial tours

<!-- Segue, highlighting lack of control -->
Whether an analyst uses PCA or the grand tour, cannot influence the basis. They cannot explore the structure identified or change the contribution of the variables. User-controlled steering is a key aspect of _manual_ tours that helps to test variable attribution.

(ref:figRadialTour-cap) A radial tour changing the contribution of `V2`. The contribution is in the direction of cluster separation. When its contribution is removed, the clusters overlap (right). Because of this, we say that `V2` is sensitive to the separation of these two species.

```{r figRadialTour, echo = F, out.width = "100%", fig.env = "figure*", fig.cap = "(ref:figRadialTour-cap)"}
if(F)
  file.edit("./paper/R/fig_biplot_scoring.r")
knitr::include_graphics("./figures/figRadialTour.pdf")
```

<!-- Manual tour -->
The manual tour [@cook_manual_1997] defines its basis path by manipulating the basis contribution of a selected variable. A manipulation dimension is appended onto the projection plane, giving a full contribution to the selected variable. The target bases are then chosen to rotate this newly created manipulation space. This manipulation space is similarly orthogonally restrained. The data is projected through its interpolated frames and rendered into an animation. When the contribution of one variable changes, the contributions of the other variables must also change, to maintain the orthonormality of the basis. A key feature of the manual tour is that it allows users to control the variable contributions to the basis. Such manipulations can be queued in advance or selected in real-time for human-in-the-loop analysis [@karwowski_international_2006]. Manual navigation is relatively time-consuming due to the vast volume of resulting view space and the abstract method of steering the projection basis. First, it is advisable to identify a basis of particular interest and then use the manual tour as a more directed, local exploration tool to explore the sensitivity of a variable's contribution to the feature of interest.

<!-- Radial tour variant -->
To simplify the task and keep its duration realistic, we consider a variant of the manual tour called a _radial_ tour. In a radial tour, the magnitude of along the radius with a fixed angle of contribution to the frame, as seen in Figure \@ref(fig:figRadialTour). The radial tour benefits from both continuity of the data alongside grand tours and user-steering via choosing the variable to rotate.

<!-- spinifex -->
Manual tours have been recently made available in the __R__ package __spinifex__ [@spyrison_spinifex_2020], which facilitates manual tour (and radial variant). It also provides an interface for a layered composition of tours and exporting to `gif` and `mp4` with __gganimate__ [@pedersen_gganimate_2020] or `html` widget with __plotly__ [@sievert_interactive_2020]. It is also compatible with tours made by __tourr__ [@wickham_tourr:_2011]. Now that we have a readily available means to produce various tours, we want to see how they fare against traditional discrete displays commonly used with PCA.


## Other animated linear projections

<!-- Link to _Rolling the dice_ -->
The work of @elmqvist_rolling_2008 allows users to interactively change the face of a local display by navigating to adjacent faces on a global overview scatterplot matrix. This offers analysts a way to geometrically explore the transition between adjacent faces of a scatterplot matrix as though rotating the face of dice at right angles. The interpolated frames between the orthogonal faces display linear combinations of three variables at varying degrees. This is what @mcdonald1982 called a _little tour_ with the addition of user control. It is a particular type of manual tour where only horizontal or vertical rotation is allowed.

<!-- Link to star coord & Orthographic star coord -->
Star Coordinates [@kandogan_star_2000] also arrive at the biplot scatterplot displays starting from the perspective of radial parallel coordinates. @lehmann_orthographic_2013 extend this idea, mapping it back to orthogonal projections. They provide a means to interpolate through PCA components, the orthogonal contributions of scatterplot matrix, and the grand tour. It also defines user-controlled interaction similar to manual and radial tours.

<!-- Link to TripAdvisorN-D -->
TripAdvisor [@nam_tripadvisornd_2012] is an interactive application that plans sequential interpolation between distant target frames. It also provides an additional global context of a subset of possible frames with glyph representation and an overview of variable attribution by summarizing the top ten principal components. It allows for use steering by using a "touchpad polygon". This touchpad allows for contribution magnitudes to be changed. This is similar to an incremental change with the manual tour.


## Empirical evaluation

<!-- Orthogonal variables -->
Some studies compare visualizations across complete contributions of variables. @chang_evaluation_2018 conducted an $n=51$ participant study comparing parallel coordinate plots and scatterplot matrix either in isolation, sequentially, or as a coordinated view. Accuracy, completion time, and eye focus were measured for six tasks. Three tasks were more accurate with scatterplot matrix and three with parallel coordinates, while the coordinated view was usually marginally more accurate than the max of the separate visuals. @cao_z-glyph_2018 compare nonstandardized line-glyph and star-glyphs with standardized variants (with and without fill under the curve). Each of the $n=18$ participants performed 72 trials across the six visuals, two levels of dimensions, and two levels of observations. Visuals with variable standardization outperformed the nonstandardized variants, and the radial star-glyph reportedly outperformed the line variant.

<!-- 2D vs D3, mostly PCA reduced -->
Other studies have investigated the relative benefits of projecting to 2- or 3D scatterplots in PCA-reduced spaces. @gracia_new_2016 conducted an $n=40$ user study comparing 2- and 3D scatterplots on traditional 2D monitors. Participants perform point classification, distance perception, and outlier identification tasks. The results are mixed and primarily have small differences. There is some evidence to suggest a lower error in distance perception from a 3D scatterplot. @wagner_filho_immersive_2018 performed an $n=30$ within-participants study on PCA reduced space using scatterplot displays between 2D on monitors, 3D on monitors, and 3D display with a head-mounted display. None of the tasks on any dataset lead to a significant difference in accuracy. However, the immersive display reduced effort and navigation, resulting in higher perceived accuracy and engagement. @sedlmair_empirical_2013 instead, use two expert coders to evaluate 75 datasets and four dimension reduction techniques across the displays of 2D scatterplots, interactive 3D scatterplots, and 2D scatterplot matrices. They suggest a tiered guidance approach finding that 2D scatterplots are often sufficient to resolve a feature. If not, try 2D scatterplots on a different dimension reduction technique before going to scatterplot matrix display or conclude a true negative. They find that interactive 3D scatterplots help in very few cases.


## Conclusion

Orthogonal visualizations either scale poorly with dimensionality or introduce an asymmetry of the variable ordering. Projections visualize the full $p$-data as fewer dimensions, typically 1-3 at a time. In linear projections, the resulting space is composed of a linear combination of the original variables that maintain variable interpretability. While nonlinear techniques distort and bend space in different ways that are hard to visualize and communicate.

Tours are linear projections that are animated over changes in the basis. There are several more-recent other methods that animate or facilitate manual change the basis that are related to tour techniques. Some quality metrics and empirical studies compare techniques but scarcely with animated methods. Below we conduct a user study to compare the radial tour with PCA and the grand tour on a variable attribution task on clustered data.


# User study {#sec:userstudy}

<!-- Overview of visual -->
The experiment was designed to assess the performance of the radial tour relative to the grand tour and PCA for interpreting the variable attribution to the separation between two clusters. <!-- Introduce experimental factors --> Data were simulated across three experimental factors: location of the cluster separation, cluster shape, and data dimensionality. Participant responses were collected using a web application and crowdsourced through prolific.co, [@palan_prolific_2018] an alternative to MTurk.


## Objective {#sec:objective}

<!-- Rational for visual levels -->
PCA will be used as a baseline for comparison as it is the most commonly used linear embedding. It will use static, discrete jumps between orthogonal components. The grand tour will act as a secondary control that will help evaluate the benefit of observation trackability between nearby animation frames but without user-control of its path. Lastly, the radial tour will be compared, which benefits from the continuity of animation and user control of the basis.

<!-- Prior expectations -->
Then for some subset of tasks, we expect to find that the radial tour performs most accurately. Conversely, there is less to be certain about the accuracy of such limited grand tours as there is no objective function in selecting the bases; it is possible that the random selection of the target bases altogether avoids the bases showing cluster separation. However, given that the data dimensionality is modest, and it is probable that the grand tour coincidentally regularly crossed bases with the correct information for the task.

<!-- Explicit hypothesis tests -->
Experimental factors and the definition of an accuracy measure are given below. The null hypothesis can be stated as:

\begin{align*}
  &H_0: \text{accuracy does not change across the visual methods} \\
  &H_\alpha: \text{accuracy does change across the visual methods}
\end{align*}


## Visual factors {#sec:standardization}

<!-- Background for methodology, application here -->
The visual methods are tested within-participants, with each visual being evaluated twice by each participant. The order in which experimental factors are experienced is randomized with the assignment, as illustrated in Figure \@ref(fig:figParmeterizationExample). Below discusses the design standardization and unique input associated with each visual.

<!-- Aesthetic standardization -->
The visualization methods were standardized wherever possible. Data were displayed as 2D scatterplots with biplots. All aesthetic values (color-blind safe colors, shapes, sizes, absence of legend, and axis titles) were constant. The variable contribution biplot was always shown left of the scatterplot embeddings with their aesthetic values consistent. What did vary between visuals were their inputs.

<!-- PCA -->
PCA allowed users to select between the top four principal components for each axis regardless of the data dimensionality (four or six). Upon changing an axis, the visual would change to the new view of orthogonal components without displaying intermediate bases.<!-- Grand tours --> There was no user input for the grand tour; users were instead shown a 15-second animation of the same randomly selected path (variables containing cluster separation were shuffled after simulation). Participants could view the same clip up to four times within the time limit.<!-- Radial tours --> Radial tours allowed participants to select the manipulation variable.<!--The complete animation of any variable takes about 20 seconds and is almost entirely in the projection frame at around six seconds.--> The starting basis was initialized to a half-clock design, where the variables were evenly distributed in half of the circle. This design was created to be variable agnostic while maximizing the independence of the variables. Selecting a new variable resets the animation where the new variable is manipulated to a complete contribution, zeroed contribution, and then back to its initial contribution. Animation and interpolation parameters were held constant across grand and radial tour (five frames per second with a step size of 0.1 radians between interpolated frames).


## Experimental factors {#sec:expfactors}

<!-- Introduction to experimental factors -->
In addition to the visual method, data are simulated across three experimental factors. First, the _location_ of the separation between clusters is controlled by mixing a signal and a noise variable at different ratios. Secondly, the _shape_ of the clusters reflects varying distributions of the data. And third, the _dimension_-ality of the data is also tested. The levels within each factor are described below, and Figure \@ref(fig:figExpFactors) gives a visual representation.

<!-- Illustration of experimental factors -->
```{r figExpFactors, out.width='100%', fig.env = "figure*", fig.cap = "Levels of the visuals and three experimental factors: location of cluster separation, the shape of clusters, and dimensionality of the sampled data."}
if(F)
  file.edit("./paper/R/fig_model_families.r")
knitr::include_graphics("./figures/figExpFactors.pdf")
```

<!-- Location mixing -->
The _location_ of the separation between the clusters is at the heart of the measure. It would be good to test a few varying levels. To test the sensitivity, a noise and signal variable are mixed at different ratios. The separation between clusters is mixed at the following percentages: 0/100% (not mixed), 33/66%, 50/50% (evenly mixed).

<!-- Shape, vc matrix -->
In selecting the _shape_ of the clusters, the convention given by @scrucca_mclust_2016 is followed. They describe 14 variants of model families containing three clusters. The model family name is the abbreviation of the clusters' respective volume, shape, and orientation. The levels are either *E*qual or *V*ary. The models EEE, EEV, and EVV are used. For instance, in the EEV model, the volume and shape of clusters are constant, while the shape's orientation varies. The EVV model is modified by moving four-fifths of the data out in a ">" or banana-like shape.

<!-- Dimensionality -->
_Dimension_-ality is tested at two modest levels: four dimensions containing three clusters and six with four clusters. Such modest dimensionality is required to limit the difficulty and search space to make the task realistic for crowdsourcing.


## Task and evaluation {#sec:task}

<!-- Segue to task and evaluation -->
With our hypothesis formulated, let us turn our attention to the task and how it is evaluated.<!-- Participant instruction --> Participants were asked to "check any/all variables that contribute more than average to the cluster separation green circles and orange triangles". This was further explained in the explanatory video as "mark any and all variable that carries more than their fair share of the weight, or one quarter in the case of four variables".  The participant instruction video can be viewed at \url{https://vimeo.com/712674984}.

<!-- Instruction and time limit -->
The instructions iterated several times in the video were: 1) use the input controls to find a frame that contains separation between the clusters of green circles and orange triangles, 2) look at the orientation of the variable contributions in the grey circle (biplot axes orientation), and 3) select all variables that contribute more than uniformed distributed cluster separation in the scatterplot. Independent of the experimental level, participants were limited to 60 seconds for each evaluation of this task. This restriction did not impact many participants as the 25th, 50th, and 75th quantiles of the response time were about 7, 21, and 30 seconds, respectively.

<!-- Evaluating measure -->
The accuracy measure of this task was designed with a couple of features in mind. 1) symmetric about the expected value, without preference for under- or over-guessing. 2) heavier than linear weight with an increasing difference from the expected value. The following measure is defined for evaluating the task.

<!-- Notation -->
Let the data $\textbf{X}_{n,~p,~k}$ be a simulation containing clusters of observations of different distributions. Where $n$ is the number of observations, $p$ is the number of variables, and $k$ indicates the observation's cluster. Cluster membership is exclusive; an observation cannot belong to more than one cluster.

<!-- W, weights -->
The weights, $w$, is a vector of the variable-wise difference between the mean of two clusters of less $1/p$, the expected cluster separation if it were uniformly distributed. Accuracy, $A$ is defined as the signed square of these weights if selected by the participant. Participant responses are a logical value for each variable --- whether or not the participant thinks each variable separates the two clusters more than uniformly distributed separation.

<!-- __v2 measure sq__ -->
\begin{equation*}
  w_{j} = \frac{
    (\overline{X}_{\cdot, j=1, k=1} - \overline{X}_{\cdot, 1, 2}, ~...~ 
    (\overline{X}_{\cdot, p, 1} - \overline{X}_{\cdot, p, 2})}
    {\sum_{j=1}^{p}(|\overline{X}_{\cdot, j, k=1} - \overline{X}_{\cdot, j, 2}|)} - \frac{1}{p}
\end{equation*} <!-- Merging to one equ is painful -->
\begin{equation*}
  A = \sum_{j=1}^{p}I(j) \cdot sign(w_j) \cdot w^2
\end{equation*}

Where $I(j)$ is the indicator function, the binary response for variable $j$. Figure \@ref(fig:figBiplotScoring) shows one projection of a simulation with its observed variable separation (wide bars), expected uniform separation (dashed line), and accuracy if selected (thin vertical lines).

```{r, figBiplotScoring, out.width="100%", fig.env = "figure*", fig.cap = "Illustration of how accuracy is measured. (L), Scatterplot and biplot of PC1 by PC4 of a simulated data set (R) illustrate cluster separation between the green circles and orange triangles. Bars indicate observed cluster separation, and (red/green) lines show the accuracy weight of the variable if selected. The horizontal dashed line has a height $1 / p$, the expected value of cluster separation. The accuracy weights equal the signed square of the difference between each variable value and the dashed line."}
if(F)
  file.edit("./paper/R/fig_biplot_scoring.r")
knitr::include_graphics("./figures/figBiplotScoring.pdf")
```


## Randomized factor assignment

<!-- Introduction -->
Now, with simulation and their artifacts in hand, this section covers how the experimental factors are assigned and demonstrate how this is experienced from the participant's perspective.

<!-- Periods, exp factor assignment -->
The study is sectioned into three periods. Each period is linked to a randomized level of visual and location. The order of dimension and shape are of secondary interest and are held constant in increasing order of difficulty; four then six dimensions and EEE, EEV, then EVV-banana, respectively.

<!-- Training and evaluation -->
Each period starts with an untimed training task at the simplest remaining experimental levels; location = 0/100%, shape = EEE, and four dimensions with three clusters. This serves to introduce and familiarize participants with input and visual differences. After the training, the participant performs two trials with the same visual and location level across the increasing difficulty of dimension and shape. The plot was removed after 60 seconds, though participants rarely reached this limit.

<!-- visual*location nested latin square -->
The order of the visual and location levels is randomized with a nested Latin square where all levels of the visuals are exhausted before advancing to the next level of location. This requires $(3!)^2 = 36$ participants to evaluate all permutations of the experimental factors once. This randomization controls for potential learning effects the participant may receive. Figure \@ref(fig:figParmeterizationExample) illustrates how an arbitrary participant experiences the experimental factors.

<!-- Nested latin square assignment -->
```{r figParmeterizationExample, out.width="100%", fig.env = "figure*", fig.cap = "Illustration of how a hypothetical participant 63 is assigned experimental factors. Each of the six visual order permutations is exhausted before iterating to the next permutation of location order."}
## This is a .pttx screen cap, .png ok.
knitr::include_graphics("./figures/figParmeterizationExample.png") 
```

<!-- Pilot study; 3 even evaluations of each -->
Through pilot studies sampled by convenience (information technology and statistics Ph.D. students attending Monash University), it was estimated that three complete evaluations are needed to power the study properly, a total of $N = 3 \times 3!^2 = 108$ participants.


## Participants {#sec:articipants}

$N = 108$ participants were recruited via prolific.co [@palan_prolific_2018]. Participants are restricted based on their claimed education requiring that they have completed at least an undergraduate degree (some 58,700 of the 150,400 users at the time). This restriction is used on the premise that linear projections and biplot displays will not be regularly used for consumption by general audiences. There is also the implicit filter that Prolific participants must be at least 18 years of age and have implicit biases of timezone, location, and language biases. Participants were compensated for their time at \pounds 7.50 per hour, whereas the mean duration of the survey was about 16 minutes. Previous knowledge or familiarity was minimal, as validated in the follow-up survey. The appendix, Section \@ref(sec:demographics), contains a heatmap distribution of age and education paneled across preferred pronouns of the participants that completed the survey, who are relatively young, well educated, and slightly more likely to identify as males.


# Results {#sec:results}

To recap, the primary response variable is accuracy, as defined in Section \@ref(sec:task). Two primary data sets were collected; the user study evaluations and the post-study survey. The former is the 108 participants with the experimental factors: visual, location of the cluster separation signal, the shape of the variance-covariance matrix, and the dimensionality of the data. Experimental factors and randomization were discussed in section \@ref(sec:expfactors). A follow-up survey was completed by 84 of these 108 people. It collected demographic information (preferred pronoun, age, and education) and subjective measures for each visual (preference, familiarity, ease of use, and confidence).

Below a battery of mixed regression models is built to examine the degree of the evidence and the size of the effects from the experimental factors. Then, Likert plots and rank-sum tests to compare the subjective measures between the visuals.


## Accuracy

<!-- Introduce regression model, explaining accuracy, and random effect term -->
To quantify the contribution of the experimental factors to the accuracy, mixed-effects models were fit. All models have a random effect term on the participant and the simulation. These terms explain the amount of error attributed to the individual participant's effect and variation due to the random sampling data.

<!-- Building a battery of models -->
In building a set of models to test, a base model with only the visual term being compared with the full linear model term and progressively interacting with an additional experimental factor. The models with three and four interacting variables are rank deficient; there is not enough varying information in the data to explain all interacting terms.

<!-- Y1 accuracy regression -->
\begin{small}
$$
\begin{array}{ll}
\textbf{Fixed effects} &\textbf{Full model} \\
\alpha                 &\widehat{Y} = \mu + \alpha_i + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha + \beta + \gamma + \delta &\widehat{Y} = \mu + \alpha_i + \beta_j + \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha \times \beta + \gamma + \delta &\widehat{Y} = \mu + \alpha_i \times \beta_j + \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha \times \beta \times \gamma + \delta &\widehat{Y} = \mu + \alpha_i \times \beta_j \times \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha \times \beta \times \gamma \times \delta &\widehat{Y} = \mu + \alpha_i \times \beta_j \times \gamma_k \times \delta_l + \textbf{Z} + \textbf{W} + \epsilon
\end{array}
$$

$$
\begin{array}{l}
\text{where} \\
~~\mu \text{, the intercept of the model} \\
~~\alpha_i \text{, visual term}~|~i\in (\text{pca, grand, radial}) \\
~~\beta_j  \text{, location term}~|~j\in (\text{0/100, 33/66, 50/50\% mix}) \\
~~\gamma_k \text{, shape term}~|~k\in (\text{EEE, EEV, EVV banana}) \\
~~\delta_l \text{, dimension term}~|~l\in \text{(4 \& 3, 6 \& 4) var \& clusters},  \\
~~\textbf{Z} \sim \mathcal{N}(0,~\tau), \text{ the random effect of participant} \\
~~\textbf{W} \sim \mathcal{N}(0,~\upsilon), \text{ the random effect of simulation} \\
~~\epsilon   \sim \mathcal{N}(0,~\sigma), \text{ the remaining error of the model}
\end{array}
$$
\end{small}
<!-- Merging environments is a headache -->

<!-- Y1 model comparisons -->
```{r marksCompTbl, fig.cap = "Use the caption arg in kable(), not this."}
if(F) ## Creation and saving at:
  file.edit("./paper/R/mixed_model_regression.rmd")
model_comp <- readRDS("./figures/modelCompLs.rds")
cn <- c("Model", "Terms", "Lvls")
colnames(model_comp[[1]])[1:3] <- cn
colnames(model_comp[[2]])[1:3] <- cn
model_comp[[1]] <- model_comp[[1]][-(2:3)]
model_comp[[2]] <- model_comp[[2]][-(2:3)]

## 1) Eval parameters
model_out <- kableExtra::kbl(
  model_comp[[1]], "latex", align = c("l", rep("r", 7)),
  booktabs = TRUE, linesep = "", escape = FALSE, ## Allow cell_spec formatting
  table.envir = "table", ## Wide tables; from `*`
  caption = "Model performance of random effect models regressing accuracy. Complex models perform better in terms of $R^2$ and RMSE, yet AIC and BIC penalize their large number of fixed effects in favor of the much simpler model containing only the visuals. Conditional $R^2$ includes error explained by the random effects, while marginal does not.") %>%
  kableExtra::kable_classic(font_size = kable_font_size,
                            latex_options = "striped")
## latex_options = "hold_position" NOR "HOLD_position" seem toi have any impact on table 1.
if(F)
  saveRDS(object = model_out, "./paper/figures/model_comp_y1.rds")
model_out
```

<!-- Y1 coefficients of ABcd -->
```{r marksCoefTbl, fig.cap = "Use the caption arg in kable(), not this."}
if(F) ## Creation and saving at:
  file.edit("./paper/R/mixed_model_regression.rmd")
coef_ls <- readRDS("./figures/modelCoefLs.rds")
cn <- c("Est", "SE", "df", "t val", "Prob", "")
colnames(coef_ls[[1]]) <- cn
colnames(coef_ls[[2]]) <- cn
rn <- c("(Intercept)", "VisGrand", "VisRadial", "Loc33/66%",
        "Loc50/50%", "ShapeEEV", "ShapeBanana", "Dim6",
        "VisGrand:Loc33/66", "VisRadial:Loc33/66",
        "VisGrand:Loc50/50", "VisRadial:Loc50/50")
rownames(coef_ls[[1]]) <- rn
rownames(coef_ls[[2]]) <- rn


## For grouping rows see:
if(F)
  browseURL("https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html#Group_rows_via_labeling")
kableExtra::kbl(
  coef_ls[[1]], format = "latex", escape = TRUE, booktabs = TRUE,
  linesep = "", digits = 5, table.envir = "table", ## Wide tables; from `*`
  caption = "The task accuracy model coefficients for $\\widehat{Y} = \\alpha \\times \\beta + \\gamma + \\delta$, with visual = pca, location = 0/100\\%, shape = EEE, and dim = 4 held as baselines. Visual being radial is the fixed term with the strongest evidence supporting the hypothesis. Interacting with the location term, there is evidence suggesting radial performs with minimal improvement for 33/66\\% location mixing.") %>%
  kableExtra::pack_rows("Factor", 2, 3) %>%
  kableExtra::pack_rows("Fixed effects", 4, 8) %>%
  kableExtra::pack_rows("Interactions", 9, 12) %>%
  kableExtra::kable_classic(font_size = kable_font_size,
                            latex_options = "striped")
```

<!-- Model selection and coefficients -->
Table \@ref(tab:marksCompTbl) compares the model summaries across increasing complexity. The $\alpha \times \beta + \gamma + \delta$ model is selected to examine in more detail as it has relatively high condition $R^2$ and not overly complex interacting terms. Table \@ref(tab:marksCoefTbl) looks at the coefficients for this model. There is strong evidence suggesting a relatively large increase in accuracy from the radial tour, though there is evidence that almost of increase is lost under 33/66% mixing.

<!-- Conditional effects of variables -->
We also want to visually examine the conditional variables in the model. Figure \@ref(fig:figMarksABcd) illustrates the violin plots of accuracy for each model term.

<!-- Violin plots and test overlay for Y1 visuals -->
```{r, figMarksABcd, out.width="100%", fig.env = "figure", fig.cap = "Violin plots of terms of the model $\\widehat{Y} = \\alpha \\times \\beta + \\gamma + \\delta$. Overlaid with global significance from the Kruskal-Wallis test and pairwise significance from the Wilcoxon test, both are non-parametric, ranked-sum tests. Viewing the marginal accuracy of the terms corroborates the primary findings that the use of the radial tour leads to a significant increase in accuracy, at least over PCA, and this effect is particularly well supported when no location mixing is applied."}
if(F) ## Creation and saving figure at:
  file.edit("./paper/R/mixed_model_regression.rmd")
knitr::include_graphics("./figures/figMarksABcd.pdf")
```


## Subjective measures

Modeling has shown us that use of the radial tour leads to a sizably improvement in the accuracy measure for this task. This is not the whole story. It is desirable to know what the users think of using the visuals. We follow the lead set by @wagner_filho_immersive_2018. They observe four subjective measures. The following were used in this study: confidence, ease of use, prior familiarity, and preference. Each of these questions were asked of all for each visual as 5-point Likert items.

<!-- Introduce subjective measures from n=84 survey responses -->
The 84 evaluations of the post-study survey are shown in Figure \@ref(fig:figSubjectiveMeasures). The figure uses Likert plots or stacked percentage bar plots, alongside violin plots with the same non-parametric, ranked sum tests previously used.

```{r figSubjectiveMeasures, out.width="100%", fig.env = "figure", fig.show="asis", fig.cap = "The subjective measures of the 84 responses of the post-study survey with five-point Likert items levels of agreement. (L) Likert plots (stacked percent bar plots) with (R) violin plots of the same measures. Violin plots are overlaid with global significance from the Kruskal-Wallis test and pairwise significance from the Wilcoxon test. Participants are more confident using the radial tour and find it easier to use than the grand tour. The radial tour is the most preferred visual."}
if(F)
  browseURL("https://bookdown.org/Rmadillo/likert/is-there-a-significant-difference.html#permutation-mann-whitney-tests")
if(F) ## Creation and saving figure at:
  file.edit("./paper/R/fig_population_demographics.r")
knitr::include_graphics("./figures/figSubjectiveMeasures_w.violin_hori.pdf")
```

There was strong evidence to support that participants preferred the radial tour to either alternative. There is less evidence that the radial tour led to more confidence and found easier to use than the grand tour. In confirmation of expectations, crowdsourced participants had low familiarity with all visuals, with no difference in mean supported.


# Discussion

<!-- Burden to visualize multivariate space -->
Data visualization is an integral part of understanding relationships in data and how models are fitted. When it comes to multivariate data giving a comprehensive view quickly becomes difficult as the dimensions become sizable. Developers and analysts have the task of choosing which visualization technique to use. We have discussed orthogonal visualization, observation-link glyphs, and then focus on linear projections of data. There are many empirical evaluations comparing visualization and display dimensionality for multivariate data. However, animated techniques are rarely compared in the literature. This user study is the first to compare the radial tour.

<!-- radial tour and hypothesis -->
The radial tour is a method for the analyst to choose a variable to alter its contribution to the basis. The animation over small changes to the basis allow the sensitivity of the structure to be assessed from the variable contribution. The hypothesis that user control over the basis and the permanence of observations between intermediate frames may lead to better perception of the variable attribution causing the separation of clusters.

<!-- Results & surprising -->
Mixed modeling analysis of the study provides strong support for this conclusion. That is, there is significant evidence suggests the use of the radial tour leads to a sizable increase in accuracy. One unexpected caveat is that mixing the location of the signal at 33/66% almost completely negates this gain. Perhaps this is because the "half-clock" basis used did not give enough weight to the variable containing the small fraction. It was also interesting to note that no level of the experimental factors alone had a significant effect on this setup. Lastly, the follow-up survey asked participants to evaluate measures of the visuals. Most notably, participants preferred the radial tour to the other visuals. Knowing that the radial tour outperforms alternatives and is the preferred choice can help inform the selection of visual methods for developers and analysts.


# Conclusion {#sec:conclusion}

<!-- Recap study -->
This paper discussed a crowdsourced within-participant user study ($n=108$) comparing the efficacy of three linear projection techniques: PCA, grand tour, and radial tour. The participants performed a supervised cluster task, explicitly identifying which variables contribute to the separation of two target clusters. This was evaluated evenly over four experimental factors. In summary, mixed model regression finds strong evidence that using the radial tour sizably increases accuracy, especially when cluster separation location is not mixed at 33/66%. The effect sizes on accuracy are large relative to the change from the other experimental factors and the random effect of data simulation, though smaller than the random effect of the participant. The radial tour was the most preferred of the three visuals.

<!-- Limitations and extensions -->
There are several implicit limitations to this study: the task, type of data, levels of the factors selected There are several ways that this study could be extended. In addition to expanding the support of the experimental factors, more exciting directions include: introducing a new task, including more visualizations, and changing the  experience level of the target population. It is difficult to achieve good coverage given the number of possible experimental factors.


# Acknowledgments {#sec:acknowledgments}

This research was supported by an Australian Government Research Training Program (RTP) scholarship. This article was created in __R__ [@r_core_team_r:_2020] and __rmarkdown__ [@xie_r_2018]. Visuals were prepared with __spinifex__[@spyrison_spinifex_2020]. We thank Jieyang Chong for his help in proofreading this article. The code, response files, their analyses, and the study application are publicly available at \url{https://github.com/nspyrison/spinifex_study}. The participant instruction video can be viewed at \url{https://vimeo.com/712674984}.


\newpage
# References

<div id="refs"></div>


\newpage
# Appendix {#sec:appendix}

```{r, child="appendix.rmd"}
```
