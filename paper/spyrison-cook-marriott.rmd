---
documentclass: jdssv
classoption: article
output:
  bookdown::pdf_document2:
    keep_tex: true
    toc: false
    template: template/jdssv_template.tex
title:
  formatted: "A study examining the benefit of the user-controlled radial tour for understanding variable contributions to structure visible in linear projections of high-dimensional data"
  plain:     "A study examining the benefit of the user-controlled radial tour for understanding variable contributions to structure visible in linear projections of high-dimensional data"
  short:     "Study on using the radial tour for understanding variable importance"
author:
  - name: Nicholas Spyrison
    affiliation: Monash University
    address: |
      | Faculty of Information Technology
      | ORCiD: 0000-0002-8417-0212
    email: \email{nicholas.spyrison@monash.edu}
    orcid: 0000-0002-8417-0212
  - name: Dianne Cook
    affiliation: Monash University
    address: |
      | Department of Econometrics & Business Statistics,
      | ORCiD: 0000-0002-3813-7155
    orcid: 0000-0002-3813-7155
  - name: Kimbal Marriott
    affiliation: Monash University
    address: |
      | Faculty of Information Technology,
      | ORCiD: 0000-0002-9813-0377
    orcid: 0000-0002-9813-0377
abstract: |
  Principal component analysis is a long-standing go-to method for exploring multivariate data. Data visualization _tours_ are a class of linear projections animated over small changes in the projection basis. The radial tour is one instance that rotates the contribution of a select variable. This paper describes a winthin participants user study evaluating the efficacy of using the radial tour in comparison with principal component analysis and the grand tour. We devise a supervised classification task where participants evaluate variable attribution of the separation between two classes. Accuracy and response time are measured as response variables. Data were collected from 108 crowdsourced participants, who performed two trials of each visual for 648 trials in total. There is strong evidence that the radial tour increases accuracy for this task. Participants also preferred to use the radial tour over alternatives for this task.
##preamble: > ## Moved to /template/jdsssv_template.tex
keywords:
  formatted: [multivariate data, exploratory data analysis, grand tour, manual tour, dimension reduction, linear projections, linear embeddings, "\\proglang{R}"]
  plain:     [multivariate data, exploratory data analysis, grand tour, manual tour, dimension reduction, linear projections, linear embeddings, R]
bibliography: spyrison-cook-marriott.bib
editor_options:
  chunk_output_type: console
header-includes:
    - \usepackage{hyperref}
    - \hypersetup{colorlinks = true, linkcolor = red, urlcolor = blue}
---
\bibliography{spyrison-cook-marriott}
```{r setup_paper, include=FALSE}
## Install latest CRAN ver of  packages used in compiling paper
if(F){ ## Manually run
  install.packages("knitr")
  install.packages("kableExtra")
  install.packages("magrittr")
  install.packages("tourr")
  install.packages("spinifex")
  install.packages("gganimate")
}
## try to fix error from kableExtra:
#Package xcolor Error: Undefined color `shadecolor'.
options(kableExtra.latex.load_packages = FALSE) 
## kableExtra also has a new error with column_spec():
# Undefined control sequence.
# <argument> >{\raggedright \arraybackslash 
if(interactive()) setwd("./paper")
require("knitr")
require("kableExtra")
require("magrittr")
## Work packages
require("tourr")
require("spinifex")
require("gganimate")
## chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  echo = FALSE,
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  cache = TRUE,
  cache.lazy = FALSE
)
```

\newpage

# Introduction

<!-- Multivariate spaces and EDA -->
Multivariate data is ubiquitous. Exploratory Data Analysis [EDA, @tukey_exploratory_1977] is essential for understanding the data and testing model assumptions. Data visualization is more robust and informative than statistical summarization alone [@anscombe_graphs_1973; @matejka_same_2017]. Focusing on hypothesis testing can result in tunnel vision of an analyst, causing them to miss visual particularities in the data [@yanai_hypothesis_2020]. Data visualization is integral to EDA and our comprehension of the data. But how do we know which methods to use to visualize data best?

<!--- black-box models-->
Models are becoming increasingly complex models involving many features and terms causing an opaqueness to their interpretability. Exploratory Artificial Intelligence (XAI, @adadi_peeking_2018; @arrieta_explainable_2020) attempt to make black-box models more transparent by offering techniques to increase their interpretability. Multivariate data visualization is a similarly important part of exploring features spaces and communicating interpretations of models [@biecek_dalex_2018; @biecek_explanatory_2021; @wickham_visualizing_2015]. 

<!-- Research gap -->
Dimension reduction is commonly used with visualization to provide informative low-dimensional summaries of high-dimensional data. There have been several user studies for dimension reduction comparing across embeddings and display dimensionality [@gracia_new_2016; @wagner_filho_immersive_2018]. There are also empirical metrics and comparisons used to describe non-linear reduction and how well and faithfully they embed the data [@bertini_quality_2011; @liu_visualizing_2017; @sedlmair_empirical_2013; @van_der_maaten_visualizing_2008]. There is an absence of studies comparing techniques for assessing variable attribution across visualization methods.


<!-- Overview of the study -->
This paper describes a crowdsourced (by prolific.co), user study conducted to assess the efficacy of different visualization methods. Cluster data is simulated under several additional experimental factors, namely: location, shape, and dimensionality. We task participants with identifying the variables attributing to the separation between two clusters. We define an accuracy measure for this task and use response time as a response variable of secondary interest. We perform mixed-model regression on these measures and find radial tour has strong evidence for a large improvement in accuracy from using radial tour over alternatives Prinipal Component Analysis (PCA). We also find evidence for relatively small differences in response time with PCA being the fastest then grand then radial.

<!-- Structure of the paper -->
The paper is structured as follows. Section \@ref(sec:background) discusses several visualization methods, including the ones compared in the study. Section \@ref(sec:userstudy) describes the experimental factors, tasks, and evaluation measures used. The results of the study are discussed in Section \@ref(sec:results). Conclusions and potential future directions are discussed in Section \@ref(sec:conclusion). The software used for the study is described in Section \@ref(sec:spinifex).


# Background {#sec:background}

Here, we discuss common multivariate techniques, including the methods before settling on PCA, grand tour, and the radial, manual tour.

## Scatterplot matrix

One could consider looking at $p$ histograms or univariate densities. This will miss features in two or more dimensions. Similarly, all combinations of pairs of variables can be viewed as a scatterplot matrix [@chambers_graphical_1983]. Figure \@ref(fig:figFactorPca) shows the first three components of simulated data as a scatterplot matrix. Looking at $p$ univariate densities or many bivariate scatterplots at once quickly becomes burdensome as dimensionality increases, and only displays information containing in 2 orthogonal dimensions, that is features that require in three dimensions will never be resolved.


## Parallel coordinates plot

<!-- PCP -->
Another common way to display multivariate data is with a parallel coordinates plot [@ocagne_coordonnees_1885], which displays observations by their quantile values for each variable with connected by lines to the quantile value in subsequent variables.

Parallel coordinates plot and other observations-based visuals such as pixel plots or Chernoff faces scales well with dimensions and but poorly with observations. These are perhaps best used when there are more variables than observations.

Observations-based visuals have a couple of issues. One is that they are asymmetric across variable ordering which can lead to different conclusions or features being focused on due to variable order. Another notable issue of observations-based visuals is the graphical channel used to convey information. Munzner suggests that position is the visual channel that is most perceptible by human perception [@munzner_visualization_2014]. In the case of parallel coordinates plots, the horizontal axes span variables rather than the values of one variable. That is the loss of using an axis, our most discerning visual channel.


## Principal component analysis

PCA is a good baseline of comparison for linear projections because of its frequent and broad use across disciplines. Principal component analysis [@pearson_liii._1901] finds new components, a linear combinations of the original variables, ordered by decreasing variation. While the full dimensionality is intact, the benefit comes from the ordered nature of the components. The data is said to be approximated but the first several components, the exact number typically being subjectively selected given the variance contained by each dimension.

```{r figFactorPca, echo = F, out.width = '60%', fig.cap = "Scatterplot matrix of the first four principal components simulated data in six dimension. An analyst would have to view PC1 by PC2 and PC1 by PC4 to have a thorough take on which variables attribute to the separation between clusters."}
if(F)
  file.edit("./paper/R/fig_pca_splom.r")
knitr::include_graphics("./figures/fig_pca_splom.pdf")
```

## Animated linear projections, tours

<!-- tours intro -->
A data visualization _tour_ animates many linear projections over small changes in the projection basis. One of the insightful features of the tour is the object permanence of the data points; one can track the relative changes of observations as the basis moves, as opposed to discretely jumping to an orthogonal view with no intermediate information. Types of tours are distinguished by the selection of their basis paths [@lee_state_2021; @cook_grand_2008]. To contrast with the discrete orientations of PCA, we compare with continuous changes of linear projection with _grand_ and _radial_ tours.

### Grand tours

<!-- Grand tour -->
In a grand tour [@asimov_grand_1985], the target bases are selected randomly. The grand tour is the first and most widely known tour. The random selection of target bases makes it a general unguided exploratory tool. It will make a good comparison that has continuity of data points in nearby frames along with the radial tour but lacks the user control enjoyed by PCA and radial tours. 

### Manual and radial tours

<!-- Segue, highlighting lack of control -->
Whether an analyst uses a component space or the grand tour they have no way of influencing the basis. They cannot explore the structure identified or change the contribution of the variables. This means of user-control-steering is a key aspect of _manual_ tours that should facilitate testing variable attribution.

<!-- Manual tour -->
The manual tour [@cook_manual_1997] defines its basis path by manipulating the basis contribution of a selected variable. A manipulation dimension is appended onto the projection plane, with a full contribution given to the selected variable. The target bases are then chosen to rotate this newly created manipulation space. The target bases are then similarly orthogonally restrained, the data is projected through interpolated frames and rendered into an animation. For the variables to remain independent of each other, the contributions of the other variables must also change, _ie._ dimension space must maintain its orthonormal structure. A key feature of the manual tour is that it allows users to control the variable contributions to the basis. This means that such manipulations can be selected and queued in advance or selected on the spot for human-in-the-loop analysis [@karwowski_international_2006]. However, this navigation is relatively time-consuming due to the vast volume of $p$-space (an aspect of the curse of dimensionality, @bellman_dynamic_1957) and the abstract method of steering the projection basis. It is advisable first to identify a basis of particular interest and then use a manual tour as more directed, local exploration tool to observe how the contribution of a variable is sensitive to the feature of interest.

<!-- Radial tour variant -->
To simplify the task and keep its duration realistic, we consider a variant of the manual tour, called a _radial_ tour. In a radial tour, the selected variable is allowed to change its magnitude of contribution but not its angle; it must move along the direction of its original contribution radius. The radial tour benefits from both continuity of the data alongside grand tours, but also allows the user to steer via choosing the variable to rotate.

<!-- spinifex -->
The recent implementation of manual tours us the R package \pkg{spinifex} [@spyrison_spinifex_2020], which facilitates manual tours (and radial variant). It is also compatible with tours made with \pkg{tourr} [@wickham_tourr:_2011] and facilitates exporting to .gif or .html widget, with recent graphic packages. Now that we have a readily available means to produce various tours, we want to see how they fare against traditional discrete displays commonly used with PCA.


# User study {#sec:userstudy}

<!-- Overview of visual -->
An experiment was constructed to assess the performance of the radial tour relative to the grand tour and PCA for interpreting the variable attribution contributing to separation between two clusters. <!-- Introduce blocks --> These three methods were applied to simulations across three experimental factors: cluster shape, location of the cluster separation, and data dimensionality. Data was collected using a specially constructed web app, through crowdsourced with prolific.co [@palan_prolific_2018].


## Objective {#sec:objective}

<!-- Rational for factor levels -->
PCA will be used as a baseline for comparison as it is the most common linear embedding. The grand tour will act as a secondary control that will help evaluate the benefit of animation but without influencing its path. Lastly, the radial tour should perform best as it benefits both from animation and being user-control of the contribution of individual variables.

<!-- Prior expectations -->
Then for some subset of tasks, we expect to find that the radial tour performs most accurately, as it enjoys the persistence of points and input control to explore specific variables. Secondly, it may be the case that grand performs faster than the alternatives as its absence of inputs will allow to focus all of their attention on interpreting the fixed path. Conversely, we are less sure about the accuracy of such limited grand tours as there is no objective function in the selection of the bases; it is possible that, by chance, the planes altogether avoid bases showing cluster separation. However, given that the data dimensionality will be modest, it seems likely that grand tour will regularly crosses frames with the correct information to perform the task.

<!-- Explicit hypothesis tests -->
We measure the accuracy and response time over the support of the discussed experimental factors. The null hypotheses can be stated as:

$~~~~H_0: y_1, \text{task accuracy does not vary with the visualization method} \\$
$~~~~~H_\alpha: y_1, \text{task accuracy does vary with the visualization method} \\$
$~~~~~H_0: y_2, \text{task response time does not vary with the visualization method} \\$
$~~~~~H_\alpha: y_2, \text{task response time does vary with the visualization method} \\$


## Experimental factors {#sec:blocks}

<!-- Introduction to blocks -->
In addition to visual factor, we vary the data across three aspects: 1) The _location_ of the difference between clusters, by mixing a signal and a noise variable at different ratios, we vary the number of variables and their magnitude of cluster separation, 2) the _shape_ of the clusters, to reflect varying distributions of the data, and 3) the _dimension_-ality of the data. Below we describe the levels within each factor, while Figure \@ref(fig:figExpFactors) gives a visual representation.

<!-- Illustration of blocks -->
```{r figExpFactors, echo = F, out.width = '100%', fig.cap = "Illustration of the experimental factors, the parameter space of the independent variables, the support of our study."}
if(F)
  file.edit("./paper/R/fig_model_families.r")
knitr::include_graphics("./figures/figExpFactors.pdf")
```

<!-- Location mixing -->
The _location_ of the separation of the clusters is a crucial aspect of analysis, it is the variables or combination their of that is important to the explanation of the structure. To test the sensitivity to this, we mix a noise variable with the signal-containing variable such that the difference in the clusters is mixed at the following percentages: 0/100% (not mixed), 33/66%, 50/50% (evenly mixed).

<!-- Shape, vc matrix -->
In selecting the _shape_ of the clusters, we follow the convention given by @scrucca_mclust_2016, where 14 variants of model families containing three clusters are defined. The name of the model family is the abbreviation of its respective volume, shape, and orientation of the cluster, which are either equal or vary. We use the models EEE, EEV, and EVV. The latter is further modified by moving four-fifths of the data out in a "V" or banana-like shape.

<!-- Dimensionality -->
_Dimension_-ality is tested at two modest levels, namely, in four dimensions containing three clusters and six dimensions with four clusters. We must do so to bound the difficulty and search space to keep the task realistic for crowdsourcing.


## Task and evaluation {#sec:task}

<!-- segue to task and evaluation -->
With our hypothesis formulated, let us turn our attention to the task and how to evaluate it. Regardless of the visual method, the elements of the display are held constant a 2D scatterplot with axis biplot to its left. Observations were supervised with the cluster level mapped to color and shape.

<!-- Geom, clusters, explicit task -->
Participants were asked to 'check any/all variables that contribute more than average to the cluster separation green circles and orange triangles', which was further explained in the explanatory video as 'mark and all variable that carries more than their fair share of the weight, or one quarter in the case of four variables'.

<!-- Instruction and video -->
The instructions iterated several times in the video was: 1) use the input controls to find a frame that contains separation between the clusters of green circles and orange triangles, 2) look at the orientation of the variable contributions in the gray circle, a visual depiction of basis, and 3) select all variables that contribute more than average in the direction of the separation in the scatterplot. Independent with factor and block values,  participants were limited to 60 seconds for each evaluation of this task. This restriction did not impact many participants as the 25th, 50th, 75th quantiles of response time were about 7, 21, and 30 seconds respectively.

<!-- Evaluating measure -->
The evaluation measure of this task was designed with a couple of features in mind: 1) the sum of squares of the individual variable weights should be one, and 2) symmetric about zero, that is, without preference to under- or over-guessing. With these in mind, we define the following measure for evaluating the task.

Let a data $\textbf{X}_{n\*p\*k}$ be a simulation containing clusters of observations of different distributions. Where $n$ is the number of observations, $p$ is the number of variables, and $k$ is the number of clusters. Cluster membership is exclusive; an observation cannot belong to more than one cluster.

<!-- W, weights -->
We define weights, $W$ to be a vector explaining the variable-wise difference between two clusters. Namely, the difference of each variable between clusters, as a proportion of the total difference, less $1/p$, the amount of difference each variable would hold if it were uniformly distributed. <!-- R, participant responses -->Participant responses are a logical value for each variable, whether or not the participant thinks each variable separates the two clusters more than uniformly distributed separation. Then $Y_1$ is a vector of variable accuracy.

\begin{align*}
W_{j} &=\frac
{(\overline{X_{j=1, k=1}} - \overline{X_{1, 2}}, ~...~
(\overline{X_{p, 1}} - \overline{X_{p, 2}})}
{\sum_{j=1}^{p}(|\overline{X_{j, k=1}} - \overline{X_{j, k=2}}|)}
- \frac{1}{p} \\
\\
\text{Accuracy}, Y &= \sum_{j=1}^{p}I(r_j) * sign(w_j) * \sqrt{|w_j|} \\
\end{align*}

Where $I$ is the indicator function. Then the task accuracy is the sum of this vector. We use the time till the last response as a secondary dependent variable $Y_2$.

```{r, figBiplotScoring, echo = F, out.width = '100%', fig.cap = "(L), PCA biplot of the components showing the most cluster separation with (R) illustration of the magnitude of cluster separation is for each variable (bars) and the weight of the variable accuracy if selected (red/green lines). The horizontal dashed line is $1 / p$, the amount of separation each variable would have if evenly distributed. The weights equal the signed square of the difference between each variable value and the dashed line."}
if(F)
  file.edit("./paper/R/fig_biplot_scoring.r")
knitr::include_graphics("./figures/figBiplotScoring.pdf")
```


## Visual design standardization {#sec:standardization}

<!-- Background for methodology, application here -->
Section \@ref(sec:background) gives the sources and descriptions of the visual factors PCA, grand tours, and radial manual tours. The factors are tested within-participant, with each factor being evaluated by each participant. The order that factors are experienced is controlled with the block assignment as illustrated below in Figure \@ref(fig:figParmeterizationExample). Below we cover the visual design standardization, as well the input and display within each factor.

<!-- Aesthetic standardization -->
The visualization methods were standardized wherever possible. Each factor was shown as a biplot, display variable contributions on a unit circle. All aesthetic values (colors, shapes, sizes, absence of legend, and absence of axis titles) were held constant. Variable contributions were always shown left of the scatterplot embeddings with their aesthetic values consistent. What did vary between factors were their inputs which caused a discrete jump to another pair or principal components. They were absent for the grand tour with target bases to animate through selected at random, or for the radial tour, which variable should have its contribution animated.

<!-- PCA -->
PCA inputs allowed for users to select between the top four principal components for both the x and y-axis regardless of the data dimensionality (either four or six). <!-- Grand tours -->There was no user input for the grand tour; users were instead shown a 15-second animation of the same randomly selected path. Participant were able to view the same clip up to four times within the time limit. <!-- Radial tours -->Radial tours were also displayed at five frames per second with an interpolation step size of 0.1 radians. Users were able to swap between variables. The display would change the start of radially increasing the contribution of the selected variable until it was full, zeroed, and then back to its initial contribution. The complete animation of any variable takes about 20 seconds and is almost entirely in the projection frame at around six seconds. The starting basis of each is initialized to a half-clock design, where the variables were evenly distributed in half of the circle which is then orthonormalized. This design was created to be variable agnostic while maximizing the independence of the variables.


## Data simulation

<!-- Clusters and correlation -->
Each dimension is originally distributed as $\mathcal{N}(2 * I(signal), 1)~|~\text{covariance}~\Sigma$, a function of the shape factor. Signal variables had a correlation of 0.9 when they have equal orientation and -0.9 when their orientations vary. Noise variables were restricted to zero correlation. Each cluster is simulated with 140 observations and is offset in a variable that did not distinguish previous variables. 
 
<!-- Apply shape and location transformations -->
Clusters of the EVV shape are transformed to the banana-chevron shape. Then location mixing is applied by post-multiplying a (2x2) rotation matrix to the signal variable and a noise variable for the clusters in question.<!-- Preprocess and replicate and save --> All variables are then standardized by standard deviation. The rows and columns are then shuffled randomly. The observation's cluster and order of shuffling are attached to the data and saved.

<!-- Iterating over factor -->
Each of these replications is then iterated with each level of the factor. For PCA, every pair of the top four principal components and saved as 12 plots. For the grand tour, we first save two basis paths of differing dimension before each replication is projected through the common basis path. At the same time, each repetition's variable order was previously shuffled. The resulting animations were saved as .gif files. The radial tour starts at either the four or six variable "half-clock" basis, where each variable has a uniform contribution and no variable contributing in the opposite direction (to minimize variable dependence), a radial tour is then produced for each variable and saved as a .gif.


## Randomized factor assignment

<!-- Introduction -->
Now, with simulation and their artifacts in hand. We explain how the experimental factors are assignment, and illustrate how this is experienced from a participant's perspective.

<!-- Periods, block assignment -->
We section the study into three periods. Each period is linked to a randomized level of factor visualization and the location. The order of dimension and shape are of secondary interest and are held constant in increasing order of difficultly; four then six dimensions and EEE, EEV, then EVV-banana, respectively.

<!-- Training and evaluation -->
Each period starts with an untimed training task at the simplest remaining block levels; location = 0/100%, shape = EEE, and four dimensions with three clusters. This serves to introduce and familiarize participants with input and visual differences. After the training, the participant is evaluated on two tasks with the same factor\*location level across the increasing difficulty of dimension\*shape. The plot was removed after 60 seconds, though this limit was rarely reached by participants.

<!-- Factor*location nested latin square -->
The order of the levels of the factor and location is randomized with a nested Latin square where all levels of the visual factor are exhausted before advancing to the next level of location. That means we need $3!^2 = 36$ participants to evaluate all permutations of the experimental factors once. This randomization is essential to control for any potential learning effects the participant may receive. Figure \@ref(fig:figParmeterizationExample) illustrates how an arbitrary participant experiences the experimental factors.

<!-- Nested latin square assignment -->
```{r figParmeterizationExample, echo = F, out.width = '100%', fig.cap = "Illustration of how a hypothetical participant 63 is assigned factor and block parameterizations. Each of the 6-factor order permutations is exhausted before iterating to the next permutation of location order."}
knitr::include_graphics("./figures/figParmeterizationExample.png") 
## This is a .pttx screen cap, .png ok.
```

<!-- Pilot study; 3 even evaluations of each -->
Through pilot studies sampled by convenience (information technology and statistics Ph.D. students attending Monash University), we predict that we need three full evaluations to properly power our study; we set out to crowdsource $N = 3 * 3!^2 = 108$ participants.


## Participants {#sec:articipants}

We recruited $N = 108$ participants via prolific.co [@palan_prolific_2018]. We filtered participants based on their claimed education requiring that they have completed at least an undergraduate degree (some 58,700 of the 150,400 users at the time); we apply this filter under the premise that linear projections and biplot displays used will not be regularly used for consumption by general audiences. There is also the implicit filter that Prolific participants must be at least 18 years of age and implicit biases  of timezone, location, and language. Participants were compensated for their time at \pounds 7.50 per hour, whereas the mean duration of the survey was about 16 minutes. We can't preclude previous knowledge or experience with the factors but validate this assumption in the follow-up survey asking about familiarity with the factors. The appendix contains a heatmap distribution of age and education paneled across preferred pronouns of the participants that completed the survey, who are relatively young, well educated, and slightly more likely to identify as males.


## Data collection

<!-- App, data collection, network issues -->
Data were recorded by a \pkg{shiny} application and were written to a Google Sheet after each third of the study. Especially at the start of the study, participants experienced adverse network conditions due to the volume of participants hitting the application with modest allocated resources. In addition to this, API read/write limitations further hindered data collection. To mitigate this, we throttled the volume of participants and over-collect survey trials until we had received our target three evaluations of all permutation levels.

<!-- Preprocessing steps -->
The processing steps were minimal. First, we format to an analysis-ready form, decoding values to a more human-readable state, and add a flag to indicate if the survey had complete data. We filter to only the latest three complete studies of each block parameterization, those which should have experienced the least adverse network conditions. The bulk of the studies removed were partial data and a few over-sampled permutations. This brings us to the 108 studies described in the paper, from which models and aggregation tables were built. The post-study surveys were similarly decoded to human-readable format and then filtered to include only those 84 surveys that were associated with the final 108 studies.

The code, response files, their analyses, and the study application are publicly available at on GitHub \url{https://github.com/nspyrison/spinifex_study}.


# Results {#sec:results}

To recap, the primary response variable is task accuracy, as defined in section \@ref(sec:task), and the log of response time will be used as a secondary response variable. We have two primary data sets; the user study evaluations and the post-study survey. The former is the 108 particpants with the explanatory variables: visual factor, location of the cluster separation signal, the shape of variance-covariance matrix, and the dimensionality of the data. Block parameterization and randomization were discussed in section \@ref(sec:blocks). The survey was completed by 84 of these 108 people. It collected demographic information (preferred pronoun, age, and education), and subjective measures for each of the factors (preference, familiarity, ease of use, and confidence).

Below we look at the marginal performance of the block parameters and survey responses. After that, we build a battery of regression models to explore the variables and their interactions. Lastly, we look at the subjective measures between the factors.


## Accuracy regression

<!-- Introduce regression model, explaining accuracy, and random effect term -->
To more thoroughly examine explanatory variables, we regress against accuracy. All models have a random effect term on the participant, which captures the effect of the individual participant. After we look at models of the block parameters, we extend to compare against survey variables. Last, we compare adding a random effect for data and regressing against time till last response fares against benchmark models. The matrices for models with more than a few terms quickly become rank deficient; there is not enough data to explain all of the effect terms. <!-- In which case, the least impactful terms are dropped. -->

<!-- Building a battery of models -->
In building a set of models to test, we include all single term models those with all independent terms. We also include an interaction term of factor by location, allowing for the slope of each location to change across each level of the factor. For comparison, an overly complex model with all interaction terms is included.

<!-- Y1 accuracy regression -->
$$
\begin{array}{ll}
\textbf{Fixed effects}           &\textbf{Full model} \\
\alpha                           &\widehat{Y_1} = \mu + \alpha_i + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha + \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i + \beta_j + \gamma_k + \delta_l + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta + \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma + \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k + \textbf{Z} + \textbf{W} + \epsilon \\
\alpha * \beta * \gamma * \delta &\widehat{Y_1} = \mu + \alpha_i * \beta_j * \gamma_k * \delta_l + \textbf{Z} + \textbf{W} + \epsilon
\end{array}
$$
$$
\begin{array}{ll}
\text{where } &\mu \text{ is the intercept of the model including the mean of random effect} \\
&\epsilon   \sim \mathcal{N}(0,~\sigma), \text{ the error of the model} \\
&\textbf{Z} \sim \mathcal{N}(0,~\tau), \text{ the random effect of participant} \\
&\textbf{W} \sim \mathcal{N}(0,~\upsilon), \text{ the random effect of simulation} \\
&\alpha_i \text{, fixed term for factor}~|~i\in (\text{pca, grand, radial}) \\
&\beta_j  \text{, fixed term for location}~|~j\in (\text{0/100\%, 33/66\%, 50/50\%}) \text{ \% noise/signal mixing} \\
&\gamma_k \text{, fixed term for shape}~|~k\in (\text{EEE, EEV, EVV banana}) \text{ model shapes} \\
&\delta_l \text{, fixed term for dimension}~|~l\in (\text{4 variables \& 3 cluster, 6 variables \& 4 clusters}) \\
\end{array}
$$
<!-- Y1 model comparisons -->
```{r marksCompTbl, fig.cap = "Use the caption arg in kable(), not this."}
if(F) ## Creation and saving at:
  file.edit("./paper/R/mixed_model_regression.rmd")
model_comp_tbl_ls <- readRDS("./figures/modelCompLs.rds")

## 1) Eval parameters
model_comp <- kableExtra::kbl(
  model_comp_tbl_ls[[1]], "latex", align = c("l", rep("l", 2), rep("c", 5)),
  booktabs = TRUE, linesep = "",
  caption = "Model performance of random effect models regressing accuracy. Each model includes a random effect term of the participant explaining the individual's influence on accuracy. Complex models perform better in terms of $R^2$ and RMSE, yet AIC and BIC penalize their large number of fixed effects in favor of the much simpler model containing only the visual factor. Conditional $R^2$ includes the random effects, while marginal does not.") %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 10)
if(F)
  saveRDS(object = model_comp,
          "./paper/figures/model_comp_y1.rds")
model_comp
#knitr::kable(model_comp_tbl_ls[[1]])
```


<!-- Y1 coefficients of ABcd -->
```{r marksCoefTbl, fig.cap = "Use the caption arg in kable(), not this."}
if(F) ## Creation and saving at:
  file.edit("./paper/R/mixed_model_regression.rmd")
coef_ls <- readRDS("./figures/modelCoefLs.rds")

## For grouping rows see:
if(F)
  browseURL("https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html#Group_rows_via_labeling")
kableExtra::kbl(coef_ls[[1]], format = "latex", booktabs = TRUE, linesep = "",
                caption = "The task accuracy model coefficients for $\\widehat{Y_1} = \\alpha * \\beta + \\gamma + \\delta$, with factor=pca, location=0/100\\%, and shape=EEE held as baselines. Factor being radial is the fixed term with the strongest evidence supporting the hypothesis. When crossing factor with location radial performs worse with 33/66\\% mixing relative to the PCA with no mixing. The model fit is based on the 648 evaluations by the 108 participants.") %>%
  kableExtra::pack_rows("Factor", 2, 3) %>%
  kableExtra::pack_rows("Fixed effects", 4, 8) %>%
  kableExtra::pack_rows("Interactions", 9, 12) %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 10)
```

<!-- Conditional effects of variables -->
We also want to visually explore the conditional variables in the model. Figure \@ref(fig:figMarksABcd) explores violin plots of accuracy by factor while faceting on the location (vertical) and shape (horizontal). Radial tends to increase the accuracy received, and especially so when there is no signal/noise mixing.

<!-- Violin plots and test overlay for Y1 factors -->
```{r, figMarksABcd, echo = F, out.width = '100%', fig.cap = "Violin plots of terms of the model $\\widehat{Y_1} = \\alpha * \\beta + \\gamma + \\delta$. Overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests suitable for handling discrete data. Participants are more confident and find radial tour easier to use than the grand tour. Participants claim low familiarity as we expect from crowdsourced participants. Radial is more preferred compared with either alternative for this task."}
if(F) ## Creation and saving figure at:
  file.edit("./paper/R/mixed_model_regression.rmd")
knitr::include_graphics("./figures/figMarksABcd.pdf")
```

## Response time regression

<!-- Time as secondary interest, Y2 -->
As a secondary explanatory variable, we also want to look at time. First, we take the log transformation of time as it is right-skewed. We repeat the same modeling procedure: 1) build a battery of all additive and multiplicative models. 2) Compare their performance, reporting some top performers. 3) Select a model to examine its coefficients.

<!-- Y2 model comparisons, continue to use ABcd -->
```{r timeCompTbl, fig.cap = "Use the caption arg in kable(), not this."}
## 1) Eval parameters
kableExtra::kbl(
  model_comp_tbl_ls[[2]], "latex", align = c("l", rep("l", 2), rep("c", 5)),
  booktabs = TRUE, linesep = "", row.names = FALSE, caption = "Model performance regressing on log response time [seconds], $\\widehat{Y_2}$ random effect models, where each model includes random effect terms for participants and simulations. We see the same trade-off where the simplest factor model is preferred by AIC/BIC, while $R^2$ and RMSE is larest in the full multiplicative model. We again select the model $\\alpha * \\beta + \\gamma + \\delta$ to explore further as it has relatively high marginal $R^2$ while having much less complexity than the complete interaction model. Conditional $R^2$ includes the random effects, while marginal does not.") %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 10)
```

<!-- Y2 coeffiecients -->
```{r timeCoefTbl, fig.cap = "Use the caption arg in kable(), not this."}
kableExtra::kbl(coef_ls[[2]], booktabs = TRUE, linesep = "", format = "latex",
                caption = "Model coefficients for log response time [seconds] $\\widehat{Y_2} = \\alpha * \\beta + \\gamma + \\delta$, with factor=PCA, location=0/100\\%, and shape=EEE held as baselines. Location=50/50\\% is the fixed term with the strongest evidence and takes less time. In contrast, the interaction term location=50/50\\%:shape=EEV has the most evidence and takes much longer on average.") %>%
  kableExtra::pack_rows("Factor", 2, 3) %>%
  kableExtra::pack_rows("Fixed effects", 4, 8) %>%
  kableExtra::pack_rows("Interactions", 9, 12) %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 10)
```

<!-- No Y2 violin plots -->


## Subjective measures

<!-- Introduce subjective measures from n=84 survey responses -->
The 84 evaluations of the post-study survey also collect four subjective measures for each factor. Figure \@ref(fig:ch4fig6) shows the Likert plots, or stacked percentage bar plots, alongside violin plots with the same non-parametric, ranked sum tests previously used. Participants preferred to use radial for this task. Participants were also more confident of their answers and found radial tours easier than grand tours. All factors have reportedly low familiarity, as expected from crowdsourced participants.

```{r figSubjectiveMeasures, out.width = "100%", fig.show = "asis", fig.cap = "The subjective measures of the 84 responses of the post-study survey, five discrete Likert scale levels of agreement (L) Likert plots (stacked percent bar plots) with (R) violin plots of the same measures. Violin plots are overlaid with global significance from the Kruskal-Wallis test, and pairwise significance from the Wilcoxon test, both are non-parametric, ranked sum tests."}
if(F)
  browseURL("https://bookdown.org/Rmadillo/likert/is-there-a-significant-difference.html#permutation-mann-whitney-tests")

if(F) ## Creation and saving figure at:
  file.edit("./paper/R/fig_population_demographics.r")
knitr::include_graphics("./figures/figSubjectiveMeasures_w.violin_hori.pdf")
```


# Conclusion {#sec:conclusion}

<!-- context -->
Data visualization is an integral part of EDA. Yet through exploration of data in high dimensions become difficult. Previous methods offer no means for an analyst to impact the projection basis. The manual tour provides a mechanism for changing the contribution of a selected variable to the basis. Giving analysts such control should facilitate the exploration of variable-level sensitivity to the identified structure. We find strong evidence that using the radial tour improves the accuracy relative to PCA or the grand tour on the supervised cluster task assigning variable attribution to the separation of the two clusters.

<!-- Recap study -->
This paper discussed an $n=108$, with-in participant user study comparing the efficacy of three linear projection techniques. The participants performed a supervised cluster task, specifically identifying which variables contribute to the separation between two target clusters. This was evaluated evenly over four experimental factors. In summary, we find strong evidence that using the radial tour leads to a sizable increase in accuracy. There is also evidence for a small change response time, with an increasing order of PCA, grand, and radial. The effect sizes on accuracy are large relative to the change from the other experimental factors, though smaller than the random effect of the participant. The radial tour was subjectively most preferred, leading to more confidence in answers, and increased ease of use than the alternatives.

<!-- Discussion and going further -->
There are several ways that this study could be extended. In addition to expanding the support of the experimental factors, more exciting directions include: changing the type of the task, visualizations used, and experience level of the target population. It is difficult to achieve good coverage given the number of possible permutations. Keep in mind the volume of traffic and low effort of responses from participants when crowdsourcing.


# Accompanying tool: radial tour application {#sec:spinifex}

To accompany this study we have produced an application to illustrate the radial tour. The \pkg{R} package, \pkg{spinifex}, [@spyrison_spinifex_2020] is free, open-source and now contains an \pkg{shiny} [@chang_shiny_2020] application that allows users to apply various preprocessing tasks and interactively explore their data via interactive radial tour. Example datasets are provided with the ability to upload data. The .html widget produced is a more interactive variant relative to the one used in the user study. Screen captures and more details are provided in the appendix. Run the following \proglang{R} code will run the application locally.

```{r getting_started, eval=FALSE, echo=TRUE}
## Download:
install.packages("spinifex", dependencies = TRUE)
## Run shiny app:
spinifex::run_app()
```

# Acknowledgments {#sec:acknowledgments}

This research was supported by an Australian government Research Training Program (RTP) scholarship. This article was created in \proglang{R} [@r_core_team_r:_2020] and \pkg{rmarkdown} [@xie_r_2018]. Visuals were prepared with \pkg{spinifex}. All packages used are available from the Comprehensive __R} Archive Network [CRAN](https://CRAN.R-project.org/). The source files for this article, application, data, and analysis can be found on [GitHub](https://github.com/nspyrison/spinifex_study/). The source code for the \pkg{spinifex} package and accompanying shiny application can be found [here](https://github.com/nspyrison/spinifex/).


# References

<div id="refs"></div>

# Appendix

```{r, child="appendix.rmd"}
```